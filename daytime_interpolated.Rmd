---
title: "model_performance_pulse"
author: "Ngoc Nguyen"
date: "2024-02-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("Functions_for_Spectral_Clustering.R")
quantile_df <- function(x) {
  return(quantile(x, 0.98))
}

median_df <- function(x) {
  return(median(x, na.rm = TRUE))
}

quantile_df_68 <- function(x) {
  return(quantile(x, 0.68))
}
library(foreach)
library(doParallel)

# Register the parallel backend
no_cores <- detectCores() - 1
registerDoParallel(cores = no_cores)

```

#Fixing pre-pulse bias using -n pre-pulse window to reestimate R0
```{r}
df_manual <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Data/Birch effect Manual Label - shortcut.csv")
df_manual$SOP <- as.Date(df_manual$SOP)
df_manual$EOP <- as.Date(df_manual$EOP)

dir <- "/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/bias-correct-68-per-Sept25/"

file_list <- list.files(dir)

metric_list <- foreach(j = 1:34, .packages = c("dplyr"), .combine = 'rbind') %dopar% {

  file_path <- paste(dir, file_list[j], sep = "")
  RECO_df <- read.csv(file_path, header = TRUE) 
  site_name <- substr(file_list[j], start = 1, stop = 6)
  df_manual_sub <- subset(df_manual, SITE_ID == site_name)
  
  tryCatch ({
  train_df <- subset(RECO_df, RECO_NT_VUT_REF > -9999 & NEE_VUT_REF_QC == 0 & TS_F_MDS_1_QC == 0 & NIGHT == 1)
  E0_df <- temp_regression(train_df, 15, 5)
  E0_est <- E0_calculation(E0_df)
  
  RECO_df_pulse_list <- list()
  
  for(i in 1:nrow(df_manual_sub)) {
    tryCatch ({
      length_pre <- length(as.Date((df_manual_sub$SOP[i]: df_manual_sub$EOP[i])))
        
      RECO_df_all <- subset(RECO_df, as.Date(Time) %in% as.Date((df_manual_sub$SOP[i] - length_pre): (df_manual_sub$SOP[i] - 1)) & RECO_NT_VUT_REF > -9999 & NEE_VUT_REF_QC == 0 & TS_F_MDS_1_QC == 0 & NIGHT == 1)
      
      RECO_df_full <- subset(RECO_df, as.Date(Time) %in% as.Date((df_manual_sub$SOP[i] - length_pre): (df_manual_sub$SOP[i] - 1)) & RECO_NT_VUT_REF > -9999 & NEE_VUT_REF_QC == 0 & TS_F_MDS_1_QC == 0)
      
    #if pulse_cluster == 1 in any pre-pulse interval, stop fixing it
    if(!any(RECO_df_full$pulse_cluster == 1)) {
      R0_df <- R0_regression(RECO_df_all, E0_est, 2, 1) 
      R0_interpolate_df <- R0_interpolate(RECO_df_full, R0_df)
      R0_interpolate_df$RECO_NT_pre_pulse <- exp(E0_est * (1/(Tref - T0) - 1/(R0_interpolate_df$TS_F_MDS_1  + CtoK - T0)))*(R0_interpolate_df$R0_interpolated)
      R0_interpolate_df$E0_est <- E0_est
      RECO_df_pulse_list[[i]] <- R0_interpolate_df
    }
  }, error = function(err) { })
    }
  
  # Combine all the individual pulse data frames into a single data frame
  all_pulse_df <- NULL
  for(k in 1:length(RECO_df_pulse_list)) {
    all_pulse_df <- rbind(all_pulse_df, RECO_df_pulse_list[[k]])
  }          
  
  # Filter the combined data frame to remove outliers in the correction factor
  all_pulse_df_sub <- subset(all_pulse_df[, c("Time_full", "RECO_NT_pre_pulse",  "R0_interpolated", "E0_est")])
           
  # Merge the pulse data with the original RECO_df data frame
  update_df <- list(all_pulse_df_sub, RECO_df) %>% reduce(full_join, by = "Time_full")  
  
  update_df$RECO_pulse[which(!(is.na(update_df$RECO_NT_pre_pulse)))] <- update_df$RECO_NT_pre_pulse[which(!(is.na(update_df$RECO_NT_pre_pulse)))]

   write.csv(update_df, paste("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/FluxPulse-68-pre-pulse-no-overlap/", site_name, ".csv", sep = ""))
}, error = function(err) { })
  return(j)
}

update_df <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/FluxPulse-68-pre-pulse-no-overlap/US-Ton.csv")
for(j in unique(update_df$Year)) {
print(ggplot(subset(update_df, Year == j & NEE_VUT_REF_QC == 0 & Month %in% c(1,2,3,4)), aes(x = as.POSIXct(Time_full))) +
    geom_point(aes(y = NEE_VUT_REF, color = as.factor(pulse_cluster)), size = 0.5, alpha = 1) +
                  geom_line(aes(y = RECO_pulse), color = "red", size = 0.7) +
      geom_bar(stat = "identity", aes(y = P_ERA), width = 0.1, color = "gray50", fill = "gray50") +
    geom_line(aes(y = RECO_NT_VUT_REF), color = "blue", size = 0.5, alpha = 1) +
    theme_bw() +
      theme(
      legend.position = "none",
          axis.text.y = element_text(size = 15),
          axis.text.x = element_text(size = 15),
          axis.title.y = element_text(size = 15),
          axis.title.y.right = element_text(size = 15, color = "white"),
          legend.key.size = unit(1, 'cm'),
          legend.text = element_text(size=13),
          panel.grid.major = element_blank() ,
          panel.grid.minor = element_blank()) +
    scale_fill_manual(values=c("black", "#008000")) +
  scale_color_manual(values=c("black", "#008000")) +
  guides(fill=guide_legend(title="New Legend Title")) +
  ylab(expression("NEE (umol "*CO[2]*" "*m^-2*s^-1*")")) + xlab("Time") + labs(title = site_name, subtitle = j))
}
```


#Partitioning Ra and Rh
```{r}
df <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/biomass_paper/data/fluxnet_format/AMF_US-Ha1_FLUXNET_FULLSET_HR_1991-2020_3-5.csv")

df <- time_convert(df)
df_sub <- subset(df, NEE_VUT_REF_QC == 0 & TS_F_MDS_1_QC == 0 & TA_F_MDS_QC == 0 & NIGHT == 1)

temp_regression_partition <- function(df, win_len, overlap_len) {
  Eh0_collection <- c()
  Eh0_RSE <- c()
  Ea0_collection <- c()
  Ea0_RSE <- c()
  E0_date <- c()
  date_list <- seq(as.Date(unique(df$Time)[1]),
                   as.Date(unique(df$Time)[length(unique(df$Time))]), 
                   by = (win_len - overlap_len))
  for( i in as.list(date_list)) {
    test_loop = subset(df, (as.Date(Time) >= i) & (as.Date(Time) <= i + days(win_len - 1)))
    if(nrow(test_loop) >= 6) { #Only choose window with >= 6 points and temp range >= 5C
      temp_range = max(test_loop$TA_F_MDS) - min(test_loop$TA_F_MDS)
      if(temp_range >= 5) {
        try({
          regression_nls <- nls(NEE_VUT_REF ~ Rh0*exp(Eh0 * (1/(Tref - T0) - 1/(TS_F_MDS_1 + CtoK - T0))) +
                                              Ra0*exp(Ea0 * (1/(Tref - T0) - 1/(TA_F_MDS + CtoK - T0))),
                                          data = test_loop, control = nls.control(warnOnly = TRUE), start = list(Rh0 = 1, Ra0 = 1, Eh0 = 100, Ea0 = 100))
          if(is.numeric(summary(regression_nls)$parameters["Eh0",2]) & is.numeric(summary(regression_nls)$parameters["Ea0",2])) {
            E0_date <- append(E0_date, i + days(win_len - 1))
            Eh0_RSE <- append(Eh0_RSE, summary(regression_nls)$parameters["Eh0",2])
            Eh0_collection <- append(Eh0_collection, coef(regression_nls)[3])
            
            Ea0_RSE <- append(Ea0_RSE, summary(regression_nls)$parameters["Ea0",2])
            Ea0_collection <- append(Ea0_collection, coef(regression_nls)[4])
            
          } else {print("produce NA of E0")}
        })
      } else {print("temp range < 5")} 
    } else {print("window < 6 points")}
  }
  Eh0_collection <- as.data.frame(Eh0_collection)
  Ea0_collection <- as.data.frame(Ea0_collection)
  Eh0_RSE <- as.data.frame(Eh0_RSE)
  Ea0_RSE <- as.data.frame(Ea0_RSE)
  E0_date <- as.data.frame(E0_date)
  E0_data_notFiltered <- data.frame(E0_date, Eh0_collection, Eh0_RSE, Ea0_collection, Ea0_RSE)
  return(E0_data_notFiltered)
}

E0_calculation_partition <- function(df) {
  df$Eh0_collection[which(df$Eh0_collection < 0)] <- 0
  df$Ea0_collection[which(df$Ea0_collection < 0)] <- 0
  df$Eh0_collection[which(df$Eh0_collection > E0_max)] <- E0_max
  df$Ea0_collection[which(df$Ea0_collection > E0_max)] <- E0_max

  ##Selection from the literature (Reichstein et al, 2005)
  df <- subset(df , Eh0_collection > E0_min & Eh0_collection < E0_max &
                 Ea0_collection > E0_min & Ea0_collection < E0_max )
  Eh0_short_term <- mean(df$Eh0_collection)
  Ea0_short_term <- mean(df$Ea0_collection)
  E0_short_term <- data.frame(Ea0_short_term, Eh0_short_term)
  return(E0_short_term)
}
E0_df <- temp_regression_partition(df_sub, 15, 5)
E0_df_shorterm <- E0_calculation_partition(E0_df)

  df = df_sub
  win_len = 10
  overlap_len = 5
  Eh0 = E0_df_shorterm$Eh0_short_term
  Ea0 = E0_df_shorterm$Ea0_short_term
  Rh0_collection <- c()
  Ra0_collection <- c()
  R0_date <- c()
  date_list <- seq(as.Date(unique(df$Time)[1]),
                   as.Date(unique(df$Time)[length(unique(df$Time))]), 
                   by = win_len - overlap_len)
  for( i in as.list(date_list)) {
    test_loop = subset(df, as.Date(Time) >= i & as.Date(Time) <= i + days(win_len - 1))
    if(nrow(test_loop) >= 3) {
      time <- centroid_day(test_loop, i)
      try({
        regression_nls <- nls(NEE_VUT_REF ~ Rh0*exp(Eh0 * (1/(Tref - T0) - 1/(TS_F_MDS_1 + CtoK - T0))) +
                                              Ra0*exp(Ea0 * (1/(Tref - T0) - 1/(TA_F_MDS + CtoK - T0))),
                                          data = test_loop, control = nls.control(warnOnly = TRUE), start = list(Rh0 = 1, Ra0 = 1))
        if(is.numeric(summary(regression_nls)$parameters["Rh0",2]) & is.numeric(summary(regression_nls)$parameters["Ra0",2])) {
          Rh0_collection <- append(Rh0_collection, coef(regression_nls)[1])
          Ra0_collection <- append(Ra0_collection, coef(regression_nls)[2])
          R0_date <- append(R0_date, time)
        }
      })
    } else {print("less than 6 points available")}
  }
  Rh0_collection <- as.data.frame(Rh0_collection)
  Ra0_collection <- as.data.frame(Ra0_collection)
  R0_date <- as.data.frame(R0_date)
  R0_df <- data.frame(Rh0_collection, Ra0_collection, R0_date)

hist(R0_collection$Rh0_collection)
Ra0_collection
R0_date

```

#Partition Ra and Rh in US-Var
```{r}
df_var <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Data/HH_data_time_converted/US-Var_HH_time_converted.csv")

#taking period from all years where NEE = Rh --> derive E0 of Rh
```

#Histogram of decaying rate
```{r}
df_manual <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Data/Birch effect Manual Label - shortcut.csv")
df_manual$SOP <- as.Date(df_manual$SOP)
df_manual$EOP <- as.Date(df_manual$EOP)

#original data of each site (uncorrected)
dir <- "/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/Random_forest/non-training-sites/site-work-ALL/drylands/"

file_list <- list.files(dir)

# Loop through each row in the df_manual_sub data frame
ini_condition_list <- c()
decay_factor_list <- c()
ini_condition_p_list <- c()
decay_factor_p_list <- c()
site_name_list <- c()
SOP_list <- c()

for(j in 1:length(file_list)) {
  ##Reading each file
  file_path <- paste(dir, file_list[j], sep = "")
  RECO_df <- read.csv(file_path, header = TRUE) 
  RECO_df <- time_convert(RECO_df)
  RECO_df <- time_adjust(RECO_df)
  RECO_df$Time <- as.Date(RECO_df$Time)
  df_EF <- daytimeEF_df(RECO_df)
  RECO_df <- full_join(RECO_df, df_EF, by= "Time")
  #reading the pulse df for each site
  site_name <- substr(file_list[j], start = 5, stop = 10)
  df_manual_sub <- subset(df_manual, SITE_ID == site_name)
  
  for(i in 1:nrow(df_manual_sub)) {
    
    # Subset the RECO_df data frame based on the SOP and EOP dates in df_manual_sub, 
    # and filter out invalid data points
    RECO_df_sub <- subset(RECO_df, as.Date(Time) %in% as.Date((df_manual_sub$SOP[i]: df_manual_sub$EOP[i])) & RECO_NT_VUT_REF > -9999 & NEE_VUT_REF_QC == 0)
    
    # Check if there are more than 2 unique time points in the subset
    if(length(unique(RECO_df_sub$Time)) > 2) {
      
      # Group the data by Time and calculate quantiles for NEE_VUT_REF and RECO_NT_VUT_REF
      df_decay <- RECO_df_sub %>% group_by(Time) %>% summarise(across(c(NEE_VUT_REF, RECO_NT_VUT_REF), quantile_df))
      
      # Create a Day_index column to represent the number of days since the start of the pulse
      df_decay$Day_index <- 1:nrow(df_decay)
      
      # Try to fit a decay function to the NEE_VUT_REF data
      tryCatch({
        fit <- nls(data = df_decay, NEE_VUT_REF ~ a * exp(-b * Day_index), 
                   start = list(a = max(df_decay$NEE_VUT_REF), b = 0.1))
        
        # Extract the initial condition and decay factor from the fitted model
        ini_condition <- summary(fit)$coefficients[1, 1]
        decay_factor <- summary(fit)$coefficients[2,1]
        
        # Extract the p-values for the coefficients
        ini_condition_p <-  summary(fit)$coefficients[1, 4]
        decay_factor_p <- summary(fit)$coefficients[2,4]
        
        #include the initial condition and decay factor into the list
        ini_condition_list <- append(ini_condition_list, ini_condition)
        decay_factor_list <- append(decay_factor_list, decay_factor)
        ini_condition_p_list <- append(ini_condition_p_list, ini_condition_p)
        decay_factor_p_list <- append(decay_factor_p_list, decay_factor_p)
        site_name_list <- append(site_name_list, site_name)
        SOP_list <- append(SOP_list, df_manual_sub$SOP[i])
        
      }, error = function(err) {}
      )
    } 
  }
}

df_decay <- data.frame(site_name_list, SOP_list, ini_condition_list, 
                       decay_factor_list, ini_condition_p_list, decay_factor_p_list)

#write.csv(df_decay, "/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/individual_pulse_decay_rate.csv")

#decay rate
png("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Figures/19_sites_manual/decay_rate_1_histogram.png",  width     = 4,
    height    = 4,
    units     = "in",
    res       = 1200,
    pointsize = 4)

ggplot(subset(df_decay, decay_factor_p_list < 0.1 & ini_condition_p_list < 0.1 & decay_factor_list >=0), 
       aes(x=decay_factor_list)) + geom_histogram(binwidth=0.05, fill="white", color= "black") +
   theme_bw() + theme(panel.grid.minor = element_blank(),  panel.grid.major = element_blank())
dev.off()

summary(subset(df_decay, decay_factor_p_list < 0.1 & ini_condition_p_list < 0.1)$decay_factor_list)

png("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Figures/19_sites_manual/decay_rate_histogram.png",  width     = 10,
    height    = 10,
    units     = "in",
    res       = 1200,
    pointsize = 4)
ggplot(subset(df_decay, decay_factor_p_list < 0.1 & ini_condition_p_list < 0.1 & decay_factor_list >=0), 
       aes(x=decay_factor_list)) + geom_histogram(binwidth=0.05, fill="white", color= "black") +
   theme_bw() + facet_wrap(~site_name_list, scales = "free")
dev.off()

```

#Check on the deltaNEE
```{r}
df_intensity <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/3vars_all_sites.csv")

png("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Figures/19_sites_manual/change_NEE_histogram.png",  width     = 10,
    height    = 10,
    units     = "in",
    res       = 1200,
    pointsize = 4)
ggplot(subset(df_intensity, !is.na(change_NEE)), 
       aes(x=change_NEE, color = SITE_ID)) + geom_histogram(binwidth=0.5, fill="white", color = "black") +
   theme_bw() + facet_wrap(~SITE_ID, scales = "free")
dev.off()
```

#test to see which bias correction code is correct (August 18)
```{r}
df_manual <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Data/Birch effect Manual Label - shortcut.csv")
df_manual$SOP <- as.Date(df_manual$SOP)
df_manual$EOP <- as.Date(df_manual$EOP)
df_stat <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/decay_stat_all_site.csv")

#original data of each site (uncorrected)
dir <- "/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/Random_forest/non-training-sites/site-work-ALL/drylands/"

file_list <- list.files(dir)

for(j in 1:length(file_list)) {
  ##Reading each file
  file_path <- paste(dir, file_list[j], sep = "")
  RECO_df <- read.csv(file_path, header = TRUE) 
  RECO_df <- time_convert(RECO_df)
  RECO_df <- time_adjust(RECO_df)
  RECO_df$Time <- as.Date(RECO_df$Time)
  df_EF <- daytimeEF_df(RECO_df)
  RECO_df <- full_join(RECO_df, df_EF, by= "Time")
  #reading the pulse df for each site
  site_name <- substr(file_list[j], start = 5, stop = 10)
  df_manual_sub <- subset(df_manual, SITE_ID == site_name)
  
  decay_factor_composite <- subset(df_stat, SITE_ID == site_name)$decay_factor

# Initialize an empty list to store the results for each pulse
  RECO_df_pulse_list <- list()

# Loop through each row in the df_manual_sub data frame
  for(i in 1:nrow(df_manual_sub)) {
    
    # Subset the RECO_df data frame based on the SOP and EOP dates in df_manual_sub, 
    # and filter out invalid data points
    RECO_df_sub <- subset(RECO_df, as.Date(Time) %in% as.Date((df_manual_sub$SOP[i]: df_manual_sub$EOP[i])) & RECO_NT_VUT_REF > -9999 & NEE_VUT_REF_QC == 0)
    
    # Check if there are more than 2 unique time points in the subset
    if(length(unique(RECO_df_sub$Time)) > 2) {
      
      # Group the data by Time and calculate quantiles for NEE_VUT_REF and RECO_NT_VUT_REF
      df_decay <- RECO_df_sub %>% group_by(Time) %>% summarise(across(c(NEE_VUT_REF, RECO_NT_VUT_REF), quantile_df))
      
      # Create a Day_index column to represent the number of days since the start of the pulse
      df_decay$Day_index <- 1:nrow(df_decay)
      
      # Initialize a column to store the correction factors
      df_decay$correction_factor <- 1
      
      # Try to fit a decay function to the NEE_VUT_REF data
      tryCatch({
        fit <- nls(data = df_decay, NEE_VUT_REF ~ a * exp(-b * Day_index), 
                   start = list(a = max(df_decay$NEE_VUT_REF), b = 0.1))
        
        # Extract the initial condition and decay factor from the fitted model
        ini_condition <- summary(fit)$coefficients[1, 1]
        decay_factor <- summary(fit)$coefficients[2,1]
        
        # Extract the p-values for the coefficients
        ini_condition_p <-  summary(fit)$coefficients[1, 4]
        decay_factor_p <- summary(fit)$coefficients[2,4]
        
        # Apply the decay correction based on the p-values
        for(d in 1:(nrow(df_decay))) {
          if(ini_condition_p < 0.1 & decay_factor_p < 0.1) {
            # If p-values are significant, use the individual pulse decay rate
            correction_factor_0 <- max(c(df_decay$NEE_VUT_REF[1], df_decay$NEE_VUT_REF[2]))/max(c(df_deca-y$RECO_NT_VUT_REF[1], df_decay$RECO_NT_VUT_REF[2]))*exp(decay_factor)
            df_decay$correction_factor[d] <- correction_factor_0*exp(- decay_factor * d)
          } else {
            # If p-values are not significant, use the composite decay rate
            correction_factor_0 <- max(c(df_decay$NEE_VUT_REF[1], df_decay$NEE_VUT_REF[2]))/max(c(df_decay$RECO_NT_VUT_REF[1], df_decay$RECO_NT_VUT_REF[2]))*exp(decay_factor_composite)
            df_decay$correction_factor[d] <- correction_factor_0*exp(- decay_factor_composite * d)
          }
        }
      }, error = function(err) { 
        # If the model fitting fails, apply the composite decay correction
        for(d in 1:(nrow(df_decay))) {
          correction_factor_0 <- max(c(df_decay$NEE_VUT_REF[1], df_decay$NEE_VUT_REF[2]))/max(c(df_decay$RECO_NT_VUT_REF[1], df_decay$RECO_NT_VUT_REF[2]))*exp(decay_factor_composite)
          df_decay$correction_factor[d] <- correction_factor_0*exp(- decay_factor_composite * d)
        }
      })
      
      # Merge the correction factors with the original subset of data
      correction_factor_df <- df_decay[, c("Time", "correction_factor")]
      RECO_df_sub_new <- list(RECO_df_sub, correction_factor_df) %>% reduce(full_join, by = "Time")
      
      # Calculate the RECO pulse using the correction factor
      RECO_df_sub_new$RECO_pulse <- RECO_df_sub_new$RECO_NT_VUT_REF * RECO_df_sub_new$correction_factor
      
      # Add the results to the list
      RECO_df_pulse_list[[i]] <- RECO_df_sub_new
    } 
  }

# Combine all the individual pulse data frames into a single data frame
all_pulse_df <- NULL
for(i in 1:length(RECO_df_pulse_list)) {
  all_pulse_df <- rbind(all_pulse_df, RECO_df_pulse_list[[i]])
}          

# Filter the combined data frame to remove outliers in the correction factor
all_pulse_df_sub <- subset(all_pulse_df[,c("Time_full", "correction_factor", "RECO_pulse")], correction_factor < 100)
         
# Merge the pulse data with the original RECO_df data frame
update_df <- list(all_pulse_df_sub, RECO_df) %>% reduce(full_join, by = "Time_full")   

# Replace missing RECO_pulse values with the original RECO_NT_VUT_REF values
update_df$RECO_pulse[which(is.na(update_df$RECO_pulse))] <- update_df$RECO_NT_VUT_REF[which(is.na(update_df$RECO_pulse))]

# Create a vector to store pulse dates
pulse_date <- c()

# Generate the pulse dates based on SOP and EOP in df_manual_sub
for(i in 1:nrow(df_manual_sub)) {
  date <- as.Date((df_manual_sub$SOP[i]):df_manual_sub$EOP[i])
  pulse_date <- append(pulse_date, date)
}

# Create a pulse_cluster column to mark pulse dates
update_df$pulse_cluster <- update_df$NEE_VUT_REF
update_df$pulse_cluster[which(update_df$Time %in% pulse_date)] <- 1
update_df$pulse_cluster[-which(update_df$Time %in% pulse_date)] <- 0

 write.csv(update_df, paste("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/bias_correct_df_Aug18/", site_name, ".csv", sep = ""))

}
```

#rerun 68-98 percentile every site, use 68th composite decay rate
#test to see which bias correction code is correct (August 18)
```{r}
library(foreach)
library(doParallel)

# Register the parallel backend
no_cores <- detectCores() - 1
registerDoParallel(cores = no_cores)

df_manual <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Data/Birch effect Manual Label - shortcut.csv")
df_manual$SOP <- as.Date(df_manual$SOP)
df_manual$EOP <- as.Date(df_manual$EOP)
df_stat <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/68th_Pulse_decay_stats.csv")
# "/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/Mean_Pulse_decay_stats.csv")

#original data of each site (uncorrected)
dir <- "/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/Random_forest/non-training-sites/site-work-ALL/drylands/"

file_list <- list.files(dir)
metric_list <- foreach(j = 1:34, .packages = c("dplyr"), .combine = 'rbind') %dopar% {

#for(j in 1:length(file_list)) {
  ##Reading each file
  file_path <- paste(dir, file_list[j], sep = "")
  RECO_df <- read.csv(file_path, header = TRUE) 
  RECO_df <- time_convert(RECO_df)
  RECO_df <- time_adjust(RECO_df)
  RECO_df$Time <- as.Date(RECO_df$Time)
  df_EF <- daytimeEF_df(RECO_df)
  RECO_df <- full_join(RECO_df, df_EF, by= "Time")
  #reading the pulse df for each site
  site_name <- substr(file_list[j], start = 5, stop = 10)
  df_manual_sub <- subset(df_manual, SITE_ID == site_name)
  
  decay_factor_composite <- subset(df_stat, SITE_ID == site_name)$decay_factor

# Initialize an empty list to store the results for each pulse
  RECO_df_pulse_list <- list()

# Loop through each row in the df_manual_sub data frame
  for(i in 1:nrow(df_manual_sub)) {
    
    # Subset the RECO_df data frame based on the SOP and EOP dates in df_manual_sub, 
    # and filter out invalid data points
    RECO_df_sub <- subset(RECO_df, as.Date(Time) %in% as.Date((df_manual_sub$SOP[i]: df_manual_sub$EOP[i])) & RECO_NT_VUT_REF > -9999 & NEE_VUT_REF_QC == 0)
    # Check if there are more than 2 unique time points in the subset
    if(length(unique(RECO_df_sub$Time)) > 2) {
      
      # Group the data by Time and calculate quantiles for NEE_VUT_REF and RECO_NT_VUT_REF
      df_decay <- RECO_df_sub %>% group_by(Time) %>% summarise(across(c(NEE_VUT_REF, RECO_NT_VUT_REF), quantile_df_68))
      df_decay_mean <-  RECO_df_sub %>% group_by(Time) %>% summarise(across(c(NEE_VUT_REF, RECO_NT_VUT_REF), quantile_df))

      # Create a Day_index column to represent the number of days since the start of the pulse
      df_decay$Day_index <- 1:nrow(df_decay)
      
      # Initialize a column to store the correction factors
      df_decay$correction_factor <- 1
      
      # Try to fit a decay function to the NEE_VUT_REF data
      tryCatch({
        fit <- nls(data = df_decay, NEE_VUT_REF ~ a * exp(-b * Day_index), 
                   start = list(a = max(df_decay$NEE_VUT_REF), b = 0.1))
        
        # Extract the initial condition and decay factor from the fitted model
        ini_condition <- summary(fit)$coefficients[1, 1]
        decay_factor <- summary(fit)$coefficients[2,1]
        
        # Extract the p-values for the coefficients
        ini_condition_p <-  summary(fit)$coefficients[1, 4]
        decay_factor_p <- summary(fit)$coefficients[2,4]
        
        # Apply the decay correction based on the p-values
        for(d in 1:(nrow(df_decay))) {
          if(ini_condition_p < 0.1 & decay_factor_p < 0.1) {
            # If p-values are significant, use the individual pulse decay rate
            correction_factor_0 <- max(c(df_decay_mean$NEE_VUT_REF[1], df_decay_mean$NEE_VUT_REF[2]))/max(c(df_decay_mean$RECO_NT_VUT_REF[1], df_decay_mean$RECO_NT_VUT_REF[2]))*exp(decay_factor)
            df_decay$correction_factor[d] <- correction_factor_0*exp(- decay_factor * d)
          } else {
            # If p-values are not significant, use the composite decay rate
            correction_factor_0 <- max(c(df_decay_mean$NEE_VUT_REF[1], df_decay_mean$NEE_VUT_REF[2]))/max(c(df_decay_mean$RECO_NT_VUT_REF[1], df_decay_mean$RECO_NT_VUT_REF[2]))*exp(decay_factor_composite)
            df_decay$correction_factor[d] <- correction_factor_0*exp(- decay_factor_composite * d)
          }
        }
      }, error = function(err) { 
        # If the model fitting fails, apply the composite decay correction
        for(d in 1:(nrow(df_decay))) {
          correction_factor_0 <- max(c(df_decay_mean$NEE_VUT_REF[1], df_decay_mean$NEE_VUT_REF[2]))/max(c(df_decay_mean$RECO_NT_VUT_REF[1], df_decay_mean$RECO_NT_VUT_REF[2]))*exp(decay_factor_composite)
          df_decay$correction_factor[d] <- correction_factor_0*exp(- decay_factor_composite * d)
        }
      })
      
      # Merge the correction factors with the original subset of data
      correction_factor_df <- df_decay[, c("Time", "correction_factor")]
      RECO_df_sub_new <- list(RECO_df_sub, correction_factor_df) %>% reduce(full_join, by = "Time")
      
      # Calculate the RECO pulse using the correction factor
      RECO_df_sub_new$RECO_pulse <- RECO_df_sub_new$RECO_NT_VUT_REF * RECO_df_sub_new$correction_factor
      
      # Add the results to the list
      RECO_df_pulse_list[[i]] <- RECO_df_sub_new
    } 
  }

# Combine all the individual pulse data frames into a single data frame
all_pulse_df <- NULL
for(i in 1:length(RECO_df_pulse_list)) {
  all_pulse_df <- rbind(all_pulse_df, RECO_df_pulse_list[[i]])
}          

# Filter the combined data frame to remove outliers in the correction factor
all_pulse_df_sub <- subset(all_pulse_df[,c("Time_full", "correction_factor", "RECO_pulse")], correction_factor < 100)
         
# Merge the pulse data with the original RECO_df data frame
update_df <- list(all_pulse_df_sub, RECO_df) %>% reduce(full_join, by = "Time_full")   

# Replace missing RECO_pulse values with the original RECO_NT_VUT_REF values
update_df$RECO_pulse[which(is.na(update_df$RECO_pulse))] <- update_df$RECO_NT_VUT_REF[which(is.na(update_df$RECO_pulse))]

# Create a vector to store pulse dates
pulse_date <- c()

# Generate the pulse dates based on SOP and EOP in df_manual_sub
for(i in 1:nrow(df_manual_sub)) {
  date <- as.Date((df_manual_sub$SOP[i]):df_manual_sub$EOP[i])
  pulse_date <- append(pulse_date, date)
}

# Create a pulse_cluster column to mark pulse dates
update_df$pulse_cluster <- update_df$NEE_VUT_REF
update_df$pulse_cluster[which(update_df$Time %in% pulse_date)] <- 1
update_df$pulse_cluster[-which(update_df$Time %in% pulse_date)] <- 0

 write.csv(update_df, paste("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/bias-correct-68-98-68k-Sept27/", site_name, ".csv", sep = ""))

 return(j)
}
#10:23 pm - 11:19 pm
```


#test to see which bias correction code is correct (August 18): Change the way calculating Beta0 to decrease the initial values
```{r}
df_manual <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Data/Birch effect Manual Label - shortcut.csv")
df_manual$SOP <- as.Date(df_manual$SOP)
df_manual$EOP <- as.Date(df_manual$EOP)
df_stat <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/Mean_Pulse_decay_stats.csv")

#original data of each site (uncorrected)
dir <- "/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/Random_forest/non-training-sites/site-work-ALL/drylands/"

file_list <- list.files(dir)

for(j in 1:length(file_list)) {
  ##Reading each file
  file_path <- paste(dir, file_list[j], sep = "")
  RECO_df <- read.csv(file_path, header = TRUE) 
  RECO_df <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/Random_forest/non-training-sites/site-work-ALL/drylands/AMF_US-Ton_FLUXNET_FULLSET_HH_2001-2021_3-5.csv")
  RECO_df <- time_convert(RECO_df)
  RECO_df <- time_adjust(RECO_df)
  RECO_df$Time <- as.Date(RECO_df$Time)
  df_EF <- daytimeEF_df(RECO_df)
  RECO_df <- full_join(RECO_df, df_EF, by= "Time")
  #reading the pulse df for each site
  site_name <- "US-Ton"
  site_name <- substr(file_list[j], start = 5, stop = 10)
  df_manual_sub <- subset(df_manual, SITE_ID == site_name)
  
  decay_factor_composite <- subset(df_stat, SITE_ID == site_name)$decay_factor
  
  
# Initialize an empty list to store the results for each pulse
  RECO_df_pulse_list <- list()

# Loop through each row in the df_manual_sub data frame
  for(i in 1:nrow(df_manual_sub)) {
    
    # Subset the RECO_df data frame based on the SOP and EOP dates in df_manual_sub, 
    # and filter out invalid data points
    RECO_df_sub <- subset(RECO_df, as.Date(Time) %in% as.Date((df_manual_sub$SOP[i]: df_manual_sub$EOP[i])) & RECO_NT_VUT_REF > -9999 & NEE_VUT_REF_QC == 0)
    # Check if there are more than 2 unique time points in the subset
    if(length(unique(RECO_df_sub$Time)) > 2) {
      
      # Group the data by Time and calculate quantiles for NEE_VUT_REF and RECO_NT_VUT_REF
      df_decay <- RECO_df_sub %>% group_by(Time) %>% summarise(across(c(NEE_VUT_REF, RECO_NT_VUT_REF), quantile_df_70))
      
      df_decay_mean <-  RECO_df_sub %>% group_by(Time) %>% summarise(across(c(NEE_VUT_REF, RECO_NT_VUT_REF), quantile_df_70))

      # Create a Day_index column to represent the number of days since the start of the pulse
      df_decay$Day_index <- 1:nrow(df_decay)
      
      # Initialize a column to store the correction factors
      df_decay$correction_factor <- 1
      df_decay$decay_factor_p <- 0
      # Try to fit a decay function to the NEE_VUT_REF data
      tryCatch({
        fit <- nls(data = df_decay, NEE_VUT_REF ~ a * exp(-b * Day_index), 
                   start = list(a = max(df_decay$NEE_VUT_REF), b = 0.1))
        
        # Extract the initial condition and decay factor from the fitted model
        ini_condition <- summary(fit)$coefficients[1, 1]
        decay_factor <- summary(fit)$coefficients[2,1]
        
        # Extract the p-values for the coefficients
        ini_condition_p <-  summary(fit)$coefficients[1, 4]
        decay_factor_p <- summary(fit)$coefficients[2,4]
        
        df_decay$decay_factor_p <- decay_factor_p
        
        # Apply the decay correction based on the p-values
        for(d in 1:(nrow(df_decay))) {
          if(ini_condition_p < 0.1 & decay_factor_p < 0.1) {
            # If p-values are significant, use the individual pulse decay rate
            correction_factor_0 <- max(c(df_decay_mean$NEE_VUT_REF[1], df_decay_mean$NEE_VUT_REF[2]))/max(c(df_decay_mean$RECO_NT_VUT_REF[1], df_decay_mean$RECO_NT_VUT_REF[2]))*exp(decay_factor)
            df_decay$correction_factor[d] <- correction_factor_0*exp(- decay_factor * d)
          } else {
            # If p-values are not significant, use the composite decay rate
            correction_factor_0 <- max(c(df_decay_mean$NEE_VUT_REF[1], df_decay_mean$NEE_VUT_REF[2]))/max(c(df_decay_mean$RECO_NT_VUT_REF[1], df_decay_mean$RECO_NT_VUT_REF[2]))*exp(decay_factor_composite)
            df_decay$correction_factor[d] <- correction_factor_0*exp(- decay_factor_composite * d)
          }
        }
      }, error = function(err) { 
        # If the model fitting fails, apply the composite decay correction
        for(d in 1:(nrow(df_decay))) {
          correction_factor_0 <- max(c(df_decay_mean$NEE_VUT_REF[1], df_decay_mean$NEE_VUT_REF[2]))/max(c(df_decay_mean$RECO_NT_VUT_REF[1], df_decay_mean$RECO_NT_VUT_REF[2]))*exp(decay_factor_composite)
          df_decay$correction_factor[d] <- correction_factor_0*exp(- decay_factor_composite * d)
          df_decay$decay_factor_p[d] <- 0
        }
      })
      
      # Merge the correction factors with the original subset of data
      correction_factor_df <- df_decay[, c("Time", "correction_factor")]
      colnames(correction_factor_df) <- c("Time", "correction_factor")
      RECO_df_sub_new <- list(RECO_df_sub, correction_factor_df) %>% reduce(full_join, by = "Time")
      
      # Calculate the RECO pulse using the correction factor
      RECO_df_sub_new$RECO_pulse <- RECO_df_sub_new$RECO_NT_VUT_REF * RECO_df_sub_new$correction_factor
      
      # Add the results to the list
      RECO_df_pulse_list[[i]] <- RECO_df_sub_new
    } 
  }

# Combine all the individual pulse data frames into a single data frame
all_pulse_df <- NULL
for(i in 1:length(RECO_df_pulse_list)) {
  all_pulse_df <- rbind(all_pulse_df, RECO_df_pulse_list[[i]])
}          

# Filter the combined data frame to remove outliers in the correction factor
all_pulse_df_sub <- subset(all_pulse_df[,c("Time_full", "correction_factor", "RECO_pulse")], correction_factor < 100)
all_pulse_df_sub$Time_full <- as.POSIXct(all_pulse_df_sub$Time_full)
RECO_df$Time_full <- as.POSIXct(RECO_df$Time_full)
# Merge the pulse data with the original RECO_df data frame
update_df <- list(all_pulse_df_sub, RECO_df) %>% reduce(full_join, by = "Time_full")   

# Replace missing RECO_pulse values with the original RECO_NT_VUT_REF values
update_df$RECO_pulse_50[which(is.na(update_df$RECO_pulse))] <- update_df$RECO_NT_VUT_REF[which(is.na(update_df$RECO_pulse))]

# Create a vector to store pulse dates
pulse_date <- c()

# Generate the pulse dates based on SOP and EOP in df_manual_sub
for(i in 1:nrow(df_manual_sub)) {
  date <- as.Date((df_manual_sub$SOP[i]):df_manual_sub$EOP[i])
  pulse_date <- append(pulse_date, date)
}

# Create a pulse_cluster column to mark pulse dates
update_df$pulse_cluster <- update_df$NEE_VUT_REF
update_df$pulse_cluster[which(update_df$Time %in% pulse_date)] <- 1
update_df$pulse_cluster[-which(update_df$Time %in% pulse_date)] <- 0

 #write.csv(update_df, paste("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/bias_correct_df_Aug18/", site_name, ".csv", sep = ""))

}
```

#Visualize site data
```{r}

site_name = "ES-LM1"
update1 <- read.csv(paste("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/FluxPulse-68-pre-pulse/", site_name, ".csv", sep = ""))

```

```{r}
for(i in unique(update1$Year)) {
print(ggplot(subset(update1, Year == i & NEE_VUT_REF_QC == 0), aes(x = as.POSIXct(Time_full))) +
    geom_point(aes(y = NEE_VUT_REF, color = as.factor(pulse_cluster)), size = 0.5, alpha = 1) +
          #facet_wrap(~Time, scales = "free") + 
            #geom_point(aes(y = RECO_pulse), color = "red", size = 1) +
                  geom_line(aes(y = RECO_pulse), color = "red", size = 0.7) +
      geom_bar(stat = "identity", aes(y = P_ERA), width = 0.1, color = "gray50", fill = "gray50") +
   #geom_line(aes(y = RECO_pulse), color = "red", size = 0.5, alpha = 1) +
    geom_line(aes(y = RECO_NT_VUT_REF), color = "blue", size = 0.5, alpha = 1) +
    theme_bw() +
      theme(
      legend.position = "none",
          axis.text.y = element_text(size = 15),
          axis.text.x = element_text(size = 15),
          axis.title.y = element_text(size = 15),
          axis.title.y.right = element_text(size = 15, color = "white"),
          legend.key.size = unit(1, 'cm'),
          legend.text = element_text(size=13), 
          panel.grid.major = element_blank() ,
          panel.grid.minor = element_blank()) +
    scale_fill_manual(values=c("black", "#008000")) +
  scale_color_manual(values=c("black", "#008000")) +
  guides(fill=guide_legend(title="New Legend Title")) +
  ylab(expression("NEE (umol "*CO[2]*" "*m^-2*s^-1*")")) + xlab("Time") + labs(title = site_name, subtitle = i))
}
#dev.off()

```
#check whther mean or 68th of composite decay rate is better
```{r}
df_mean <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/Mean_Pulse_decay_stats.csv")
df_68th <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/68th_Pulse_decay_stats.csv")

df_mean_sub <- subset(df_mean, SITE_ID == "US-Jo2")
df_68th_sub <- subset(df_68th, SITE_ID == "US-Jo2")

ggplot(df_mean_sub) + geom_point(aes(Day_index, NEE_VUT_REF)) + theme_bw() + ylim(-1.5, 0.5)
ggplot(df_68th_sub) + geom_point(aes(Day_index, NEE_VUT_REF)) + theme_bw()+ ylim(-1.5, 0.5)

df_mean_sub
df_68th_sub
```


#Sept 28: Time_0.5_Temp
#Try out the new formula for RECO_pulse = Beta x f(Temp) >< Beta x Reco_NT
```{r}
df_manual <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Data/Birch effect Manual Label - shortcut.csv")
df_manual$SOP <- as.Date(df_manual$SOP)
df_manual$EOP <- as.Date(df_manual$EOP)

dir <- "/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/Random_forest/non-training-sites/site-work-ALL/dry_sub_4/"

file_list <- list.files(dir)

#metric_list <- foreach(j = 1:4, .packages = c("dplyr"), .combine = 'rbind') %dopar% {

  file_path <- paste(dir, file_list[j], sep = "")
  RECO_df <- read.csv(file_path, header = TRUE) 
  
  RECO_df <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/Random_forest/non-training-sites/site-work-ALL/drylands/FLX_ES-LM1_FLUXNET2015_FULLSET_HH_2014-2020_beta-3.csv")
  RECO_df <- time_convert(RECO_df)
  #RECO_df <- time_adjust(RECO_df)
  RECO_df$Time <- as.Date(RECO_df$Time)
  df_EF <- daytimeEF_df(RECO_df)
  RECO_df <- full_join(RECO_df, df_EF, by= "Time")
  
  #reading the pulse df for each site
  site_name <- "ES-LM1"
    #substr(file_list[j], start = 5, stop = 10)
  df_manual_sub <- subset(df_manual, SITE_ID == site_name)
  
  
  train_df <- subset(RECO_df, RECO_NT_VUT_REF > -9999 & NEE_VUT_REF_QC == 0 & TS_F_MDS_1_QC == 0 & NIGHT == 1)
  E0_df <- temp_regression(train_df, 15, 5)
  E0_est <- E0_calculation(E0_df)
  
  #model based on this function: Reco(d,h) = a*d^-0.5*f(Temp)

  RECO_df_pulse_list <- list()
  
  for(i in 1:nrow(df_manual_sub)) {
    tryCatch ({
    RECO_df_all <- subset(RECO_df, as.Date(Time) %in% as.Date((df_manual_sub$SOP[i]: df_manual_sub$EOP[i])) & RECO_NT_VUT_REF > -9999 & NEE_VUT_REF_QC == 0 & TS_F_MDS_1_QC == 0)

    if(length(unique(RECO_df_all$Time)) > 2) {
      df_decay <- RECO_df_all %>% group_by(Time) %>% summarise(across(c(NEE_VUT_REF, RECO_NT_VUT_REF), mean))
      df_decay$Day <- 1:nrow(df_decay)
      RECO_df_all_index <- list(RECO_df_all, df_decay[, c("Time", "Day")]) %>% reduce(full_join, by = "Time")
      RECO_df_all_index$E0_est <- E0_est

     #interpolate before and after pulses
        
        fit <- nls(data = RECO_df_all_night, NEE_VUT_REF ~ (a * exp(Day**-0.5) + b*Day) * exp(E0_est * (1/(Tref - T0) - 1/(TS_F_MDS_1 + CtoK - T0))), 
                   control = nls.control(warnOnly = TRUE), start = list(a = 3, k = 0.1)) 
       
        alpha_est <- summary(fit)$coefficients[1, 1]
        k_est <- summary(fit)$coefficients[2, 1]
        alpha_p <- summary(fit)$coefficients[1, 4]
        k_p <- summary(fit)$coefficients[2, 4]
        RECO_df_all_index$alpha <- abs(alpha_est)
        RECO_df_all_index$k <- abs(k_est)
        RECO_df_all_index$alpha_p <- abs(alpha_p)
        RECO_df_all_index$k_p <- abs(k_p)
        # Calculate the RECO pulse using the correction factor
        RECO_df_all_new <- RECO_df_all_index
        
        RECO_df_all_new$RECO_pulse <-  RECO_df_all_new$alpha*exp(-RECO_df_all_new$Day*RECO_df_all_new$k)*exp(RECO_df_all_new$E0_est * (1/(Tref - T0) - 1/(RECO_df_all_new$TS_F_MDS_1 + CtoK - T0)))
        
        # Add the results to the list
        RECO_df_pulse_list[[i]] <- RECO_df_all_new
      
    } 
  }, error = function(err) { })
    }
  
  # Combine all the individual pulse data frames into a single data frame
  all_pulse_df <- NULL
  for(i in 1:length(RECO_df_pulse_list)) {
    all_pulse_df <- rbind(all_pulse_df, RECO_df_pulse_list[[i]])
  }          
  
  # Filter the combined data frame to remove outliers in the correction factor
  all_pulse_df_sub <- subset(all_pulse_df[,c("Time_full", "RECO_pulse",  "E0_est", "alpha")])
           
  # Merge the pulse data with the original RECO_df data frame
  update_df <- list(all_pulse_df_sub, RECO_df) %>% reduce(full_join, by = "Time_full")   
  
   # write.csv(update_df, paste("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/bias_correct_df_Aug18/", site_name, ".csv", sep = ""))
  # Replace missing RECO_pulse values with the original RECO_NT_VUT_REF values
  #update_df$RECO_pulse[which(is.na(update_df$RECO_pulse))] <- update_df$RECO_NT_VUT_REF[which(is.na(update_df$RECO_pulse))]
  
  # Create a vector to store pulse dates
  pulse_date <- c()
  
  # Generate the pulse dates based on SOP and EOP in df_manual_sub
  for(i in 1:nrow(df_manual_sub)) {
    date <- as.Date((df_manual_sub$SOP[i]):df_manual_sub$EOP[i])
    pulse_date <- append(pulse_date, date)
  }
  
  # Create a pulse_cluster column to mark pulse dates
  update_df$pulse_cluster <- update_df$NEE_VUT_REF
  update_df$pulse_cluster[which(update_df$Time %in% pulse_date)] <- 1
  update_df$pulse_cluster[-which(update_df$Time %in% pulse_date)] <- 0
  
#   write.csv(update_df, paste("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/FluxPulse_model_t_Sept28/", site_name, ".csv", sep = ""))
#   return(j)
# }

for(i in unique(update_df$Year)) {
print(ggplot(subset(update_df, Year == i & NEE_VUT_REF_QC == 0 & (k_p < 0.05)), aes(x = as.POSIXct(Time_full))) +
    geom_point(aes(y = NEE_VUT_REF, color = as.factor(pulse_cluster)), size = 0.5, alpha = 1) +
          #facet_wrap(~Time, scales = "free") + 
            #geom_point(aes(y = RECO_pulse), color = "red", size = 1) +
                  geom_line(aes(y = RECO_pulse), color = "red", size = 0.7) +
      geom_bar(stat = "identity", aes(y = P_ERA), width = 0.1, color = "gray50", fill = "gray50") +
   #geom_line(aes(y = RECO_pulse), color = "red", size = 0.5, alpha = 1) +
    geom_line(aes(y = RECO_NT_VUT_REF), color = "blue", size = 0.5, alpha = 1) + #facet_wrap(~Time, scales = "free") +
    theme_bw() +
      theme(
      legend.position = "none",
          axis.text.y = element_text(size = 15),
          axis.text.x = element_text(size = 15),
          axis.title.y = element_text(size = 15),
          axis.title.y.right = element_text(size = 15, color = "white"),
          legend.key.size = unit(1, 'cm'),
          legend.text = element_text(size=13), 
          panel.grid.major = element_blank() ,
          panel.grid.minor = element_blank()) +
    scale_fill_manual(values=c("black", "#008000")) +
  scale_color_manual(values=c("black", "#008000")) +
  guides(fill=guide_legend(title="New Legend Title")) +
  ylab(expression("NEE (umol "*CO[2]*" "*m^-2*s^-1*")")) + xlab("Time") + labs(title = site_name, subtitle = i))
}
```



#functionfor calculating pulse intensity
```{r}
Initial_NEE_EF <- function(x) {
  daily_mean <- x %>%
    group_by(Time_adj) %>%
    summarise(NEE_VUT_REF = mean(NEE_VUT_REF, na.rm = TRUE))
  # Calculate the rolling mean of 2 on the shifted data
  daily_mean <- daily_mean %>%
    mutate(Before_NEE = rollmean(NEE_VUT_REF, 2, fill = NA, align = "right", na.rm = TRUE) %>% lag(2),
           After_NEE = rollmean(NEE_VUT_REF, 2, fill = NA, align = "right", na.rm = TRUE) %>% lead(0))
  
  # Calculate the Initial_EF
  daily_mean <- daily_mean %>%
    mutate(Initial_NEE = After_NEE - Before_NEE)
  
    daily_mean_EF <- x %>%
    group_by(Time_adj) %>%
    summarise(Daily_EF = mean(Daily_EF, na.rm = TRUE))
  # Calculate the rolling mean of 2 on the shifted data
  daily_mean_EF <- daily_mean_EF %>%
    mutate(Before_EF = rollmean(Daily_EF, 2, fill = NA, align = "right", na.rm = TRUE) %>% lag(2),
           After_EF = rollmean(Daily_EF, 2, fill = NA, align = "right", na.rm = TRUE)%>% lead(0))
  
  # Calculate the Initial_EF
  daily_mean_EF <- daily_mean_EF %>%
    mutate(Initial_EF = After_EF - Before_EF)
  
  # Merge the original data with the daily_mean data frame
  x_final <- merge(x, daily_mean[, c("Time_adj", "Initial_NEE")], by = "Time_adj", all.x = TRUE)
  x_final2 <- merge(x_final, daily_mean_EF[, c("Time_adj", "Initial_EF")], by = "Time_adj", all.x = TRUE)

  # Drop NA values

  return(x_final2)
}
```


```{r}
df_manual <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Data/Birch effect Manual Label - shortcut.csv")
df_manual$SOP <- as.Date(df_manual$SOP)
df_manual$EOP <- as.Date(df_manual$EOP)
site_name = "ES-LM1"
update <- read.csv(paste("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/FluxPulse-68-pre-pulse-no-overlap/", site_name, ".csv", sep = ""))

update$Time_full <- as.POSIXct(update$Time_full, format="%Y-%m-%d %H:%M:%S")
update$Time <- as.Date(update$Time_full)
update <- update %>% arrange(Time_full)

df_manual_sub <- subset(df_manual, SITE_ID == site_name)
Index_NEE_df <- Initial_NEE_EF(subset(update, NEE_VUT_REF_QC == 0))
```


#Examine Reco pulse and relationship with Daily EF
```{r}
df_cum_list <- list()
for(i in 1:length(df_manual_sub$SOP)) {
  time <- as.Date(df_manual_sub$SOP[i] : df_manual_sub$EOP[i])
  index <- 1:length(time)
  time_df <- data.frame(time, index)
  colnames(time_df) <- c("Time_adj", "Index")
  Index_NEE_df$Time_adj <- as.Date(Index_NEE_df$Time_adj)
  merge_df_sub <- list(Index_NEE_df, time_df) %>% reduce(inner_join, by = "Time_adj")
  
  merge_df_sub <- subset(merge_df_sub, NEE_VUT_REF_QC == 0 & !is.na(Daily_EF) & !is.na(Initial_NEE)  & !is.na(Initial_EF))
  
  df_cum <- data.frame(unique(merge_df_sub$Time_adj), unique(merge_df_sub$Index), unique(merge_df_sub$Daily_EF), unique(merge_df_sub$Initial_NEE), unique(merge_df_sub$Initial_EF))
  
  colnames(df_cum)<- c("Time", "Index", "Daily_EF", "Initial_NEE",  "Initial_EF")
  
  df_cum$Index_minus_sqrt <- (df_cum$Index**-0.5)
  if(nrow(df_cum) > 2) {
  df_cum$k <- coef(summary(lm(Daily_EF ~ Index_minus_sqrt, df_cum)))[2, 1]
  df_cum$p_value <- coef(summary(lm(Daily_EF ~ Index_minus_sqrt, df_cum)))[2, 4]
  df_cum$r2 <- summary(lm(Daily_EF ~ Index_minus_sqrt, df_cum))$r.squared
  df_cum$SOP <- df_manual_sub$SOP[i]
  df_cum_list[[i]] <- df_cum
  }
}

df_EF_cumm <- do.call(rbind, df_cum_list)

mean_k <- mean(subset(df_EF_cumm)$k)
df_EF_cumm$k[which(df_EF_cumm$k < 0)] <- mean_k

for(i in 1:nrow(df_EF_cumm)) { 
  if(df_EF_cumm$Index[i] != 1) {
    df_EF_cumm$Initial_NEE[i] = NA
    df_EF_cumm$Initial_EF[i] = NA
  }
}

#calculating Rh based on the obtained k
df_sub_list <- list()
for(i in unique(df_EF_cumm$SOP)) {
  df_sub <- subset(df_EF_cumm, SOP == i)
  df_sub$Rh <- df_sub$Initial_NEE[1]/df_sub$Initial_EF[1]*df_sub$k*(df_sub$Index**-0.5)
  df_sub_list[[i]] <- df_sub
}
df_Rh <- do.call(rbind, df_sub_list)

df_Rh$Time <- as.Date(df_Rh$Time)

#daily aggregate of all days from a year
Index_NEE_df$Time <- Index_NEE_df$Time_adj
df_mean <- Index_NEE_df[, c("Year", "Month", "Time", "NEE_VUT_REF", "Daily_EF", "RECO_NT_VUT_REF", "GPP_NT_VUT_REF", "pulse_cluster", "RECO_pulse")] %>%
  mutate(Date = as.Date(Time)) %>%  
  group_by(Date) %>%    
  summarise(across(everything(), mean, na.rm = TRUE)) 

df_merge <- list(df_mean, subset(df_Rh[, c("Time", "Rh", "p_value", "r2", "Index")])) %>%  reduce(full_join, by = "Time")
#plot the time series
ggplot(subset(df_merge, Year == 2017 & Month %in% c(4,5,6,7,8,9,10,11)), aes(x = as.POSIXct(Time))) + geom_point(aes(y = NEE_VUT_REF, color = as.factor(pulse_cluster)), size = 1) + theme_bw()  + geom_line(aes(y = Rh), color = "red") + 
  scale_color_manual(values=c("gray50", "#00a087")) + theme(legend.position = "none")

```

#srdb
```{r}
df_soil <- read.csv("/Users/ngocnguyen/Downloads/datSsN5Vk.csv")
df_soil$Time <- as.Date(df_soil$datetime)
df_soil_mean <- df_soil[, c("Time", "TZ_soilco2.CO2E_1")] %>%
  mutate(Date = as.Date(Time)) %>%  
  group_by(Date) %>%    
  summarise(across(everything(), mean, na.rm = TRUE)) 

df_soil_merge <- list(df_merge, df_soil_mean) %>%  reduce(full_join, by = "Time")

df_soil_merge_sub <- subset(df_soil_merge, pulse_cluster == 1)

ggplot(subset(df_soil_merge, Year == 2015)) + geom_point(aes(Rh, TZ_soilco2.CO2E_1, color = pulse_cluster)) + theme_bw()

unique(df_soil$VR_fd.f_FD_W_CO2_1)
```

#model Rh = EF*Initial_NEE/Initial_EF

```{r}
RECO_df_pulse_list <- list()

for(i in 1:nrow(df_manual_sub)) {
  RECO_df_sub <- subset(RECO_df, as.Date(Time) %in% as.Date((df_manual_sub$SOP[i]: df_manual_sub$EOP[i])) & RECO_NT_VUT_REF > -9999 & NEE_VUT_REF_QC == 0 & TS_F_MDS_1_QC == 0)
  
    RECO_df_1day <- subset(RECO_df, as.Date(Time) %in% as.Date((df_manual_sub$SOP[i]- 1): (df_manual_sub$SOP[i])) & RECO_NT_VUT_REF > -9999 & NEE_VUT_REF_QC == 0 & TS_F_MDS_1_QC == 0)
    
  if(length(unique(RECO_df_sub$Time)) > 2) {
    df_decay <- RECO_df_sub %>% group_by(Time) %>% summarise(across(c(NEE_VUT_REF, RECO_NT_VUT_REF), quantile_df))
    df_decay$Day_index <- 1:nrow(df_decay)
    df_decay$E0_est <- E0_est
    
   

    df_decay$correction_factor <- 1
      
    tryCatch({
        fit <- nls(data = df_decay, NEE_VUT_REF ~ a * exp(-b * Day_index), 
                   start = list(a = max(df_decay$NEE_VUT_REF), b = 0.1))
            regression_nls <- nls(NEE_VUT_REF ~ R0*exp(E0_est * (1/(Tref - T0) - 1/(TS_F_MDS_1 + CtoK - T0))),
                                data = RECO_df_1day, control = nls.control(warnOnly = TRUE), start = list(R0 = 3)) 
     corr_factor <- coef(regression_nls)[1]
        # Extract the initial condition and decay factor from the fitted model
        ini_condition <- summary(fit)$coefficients[1, 1]
        decay_factor <- summary(fit)$coefficients[2,1]
        
        # Extract the p-values for the coefficients
        ini_condition_p <-  summary(fit)$coefficients[1, 4]
        decay_factor_p <- summary(fit)$coefficients[2,4]

        # Apply the decay correction based on the p-values
        for(d in 1:(nrow(df_decay))) {
          if(decay_factor_p < 0.1) {
            # If p-values are significant, use the individual pulse decay rate
            
            correction_factor_0 <- corr_factor*exp(decay_factor)

            df_decay$correction_factor[d] <- correction_factor_0*exp(- decay_factor * d)
          } else {
            # If p-values are not significant, use the composite decay rate
            correction_factor_0 <- corr_factor*exp(decay_factor_composite)
            
            df_decay$correction_factor[d] <- correction_factor_0*exp(- decay_factor_composite * d)
          }
        }
      }, error = function(err) { 
      #If the model fitting fails, apply the composite decay correction
      for(d in 1:(nrow(df_decay))) {
          correction_factor_0 <- corr_factor*exp(decay_factor_composite)
          df_decay$correction_factor[d] <- correction_factor_0*exp(- decay_factor_composite * d)
      }
      }
  )
      
      # Merge the correction factors with the original subset of data
      correction_factor_df <- df_decay[, c("Time", "correction_factor", "E0_est")]
      colnames(correction_factor_df) <- c("Time", "correction_factor_temp", "E0_est")
      RECO_df_sub_new <- list(RECO_df_sub, correction_factor_df) %>% reduce(full_join, by = "Time")
      
      # Calculate the RECO pulse using the correction factor
      RECO_df_sub_new$RECO_pulse_temp <- RECO_df_sub_new$correction_factor_temp*exp(RECO_df_sub_new$E0_est * (1/(Tref - T0) - 1/(RECO_df_sub_new$TS_F_MDS_1 + CtoK - T0)))
      # Add the results to the list
      RECO_df_pulse_list[[i]] <- RECO_df_sub_new
    } 
  }

# Combine all the individual pulse data frames into a single data frame
all_pulse_df <- NULL
for(i in 1:length(RECO_df_pulse_list)) {
  all_pulse_df <- rbind(all_pulse_df, RECO_df_pulse_list[[i]])
}          

# Filter the combined data frame to remove outliers in the correction factor
all_pulse_df_sub <- subset(all_pulse_df[,c("Time_full", "correction_factor_temp", "E0_est", "RECO_pulse_temp")], correction_factor_temp < 100)
         
# Merge the pulse data with the original RECO_df data frame
update_df <- list(all_pulse_df_sub, RECO_df) %>% reduce(full_join, by = "Time_full")   

 # write.csv(update_df, paste("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/bias_correct_df_Aug18/", site_name, ".csv", sep = ""))
# Replace missing RECO_pulse values with the original RECO_NT_VUT_REF values
update_df$RECO_pulse_temp[which(is.na(update_df$RECO_pulse_temp))] <- update_df$RECO_NT_VUT_REF[which(is.na(update_df$RECO_pulse_temp))]

# Create a vector to store pulse dates
pulse_date <- c()

# Generate the pulse dates based on SOP and EOP in df_manual_sub
for(i in 1:nrow(df_manual_sub)) {
  date <- as.Date((df_manual_sub$SOP[i]):df_manual_sub$EOP[i])
  pulse_date <- append(pulse_date, date)
}

# Create a pulse_cluster column to mark pulse dates
update_df$pulse_cluster <- update_df$NEE_VUT_REF
update_df$pulse_cluster[which(update_df$Time %in% pulse_date)] <- 1
update_df$pulse_cluster[-which(update_df$Time %in% pulse_date)] <- 0
```


```{r}
update <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/FluxPulse-68-pre-pulse-no-overlap/ES-Abr.csv")
update <- update %>% arrange(Time_full)

png("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Figures/19_sites_manual/NEE_time_series_ES-Abr-2017-sub-Oct12.png",  width     = 5,
    height    = 2,
    units     = "in",
    res       = 1200,
    pointsize = 4)
# 
# ggplot(subset(update, Year == 2017 & NEE_VUT_REF_QC == 0 & DOY > 160 & DOY < 255 & NEE_VUT_REF > -5 & NEE_VUT_REF < 10 & Month %in% c(6,7,8, 9)), aes(x = as.POSIXct(Time_full))) +
#     geom_line(aes(y = Daily_EF), color = "gray50") +
#     geom_bar(data = subset(update, Year == 2017 & DOY > 160 & DOY < 255 & P_ERA > 0.001 & NEE_VUT_REF_QC == 0 & Month %in% c(6,7,8,9) & NEE_VUT_REF > -5 & NEE_VUT_REF < 10), stat = "identity", aes(y = P_ERA/2), color = "blue", position = "dodge") +
#     scale_y_continuous(sec.axis = sec_axis(~.*2, name = "Precipitation (mm/half-hour)")) +
#     theme_bw() +
#     theme(axis.text.y.right = element_text(color = "blue"),
#           axis.title.y.right = element_text(color = "blue"),
#           axis.text.y = element_text(size = 15),
#           axis.text.x = element_text(size = 15),
#           axis.title.y = element_text(size = 15),
#           axis.title.x = element_text(size = 15),
#           panel.grid.major = element_blank() ,
#           panel.grid.minor = element_blank()) +
#     ylab("Evaporative Fraction (EF)") + xlab("Time")

ggplot(subset(update, Year == 2017 & NEE_VUT_REF_QC == 0 & DOY > 160 & DOY < 255& NEE_VUT_REF > -5 & NEE_VUT_REF < 10 & Month %in% c(6,7,8, 9)), aes(x = as.POSIXct(Time_full))) +
    geom_point(aes(y = NEE_VUT_REF, color = as.factor(pulse_cluster)), size = 0.35, alpha = 0.5) +
          geom_line(aes(y = RECO_pulse), color = "red", size = 0.5) +
    geom_line(aes(y = RECO_NT_VUT_REF), color = "blue", size = 0.5) +
    theme_bw() +
      theme(
          legend.position = "none",
          axis.text.y = element_text(size = 15),
          axis.text.x = element_text(size = 15),
          axis.title.y = element_text(size = 15),
          axis.title.y.right = element_text(size = 15, color = "white"),
          legend.key.size = unit(1, 'cm'),
          legend.text = element_text(size=13),
          panel.grid.major = element_blank() ,
          panel.grid.minor = element_blank()) +
    scale_fill_manual(values=c("gray50", "#00a087")) +
  scale_color_manual(values=c("gray50", "#00a087")) +
  guides(fill=guide_legend(title="New Legend Title")) +
  ylab(expression("Half-Hour NEE (umol "*CO[2]*" "*m^-2*s^-1*")")) + xlab("Time")

dev.off()

```
#testing Russ Scott's site
#define function
```{r}
daytimeEF_df_BASE <- function(df) {
  df <- subset(df, NEE_PI > -9999 & RECO_PI > -9999 & LE > -9999 & H > -9999 & SW_IN > 20)
    df$EF <- df$LE/(df$LE + df$H)
    df_EF <- aggregate(EF ~ Time, data = df, mean)
    colnames(df_EF) <- c("Time", "Daily_EF")
    df_EF$Time <- as.Date(df_EF$Time)
    return(df_EF)
}
temp_regression <- function(df, win_len, overlap_len) {
  E0_collection <- c()
  E0_RSE <- c()
  E0_date <- c()
  date_list <- seq(as.Date(unique(df$Time)[1]),
                   as.Date(unique(df$Time)[length(unique(df$Time))]), 
                   by = (win_len - overlap_len))
  for( i in as.list(date_list)) {
    test_loop = subset(df, (as.Date(Time) >= i) & (as.Date(Time) <= i + days(win_len - 1)))
    if(nrow(test_loop) >= 6) { #Only choose window with >= 6 points and temp range >= 5C
      temp_range = max(test_loop$TA_1_1_1) - min(test_loop$TA_1_1_1)
      if(temp_range >= 5) {
        try({
          regression_nls <- nls(NEE_VUT_REF ~ R0*exp(E0 * (1/(Tref - T0) - 1/(TA_1_1_1 + CtoK - T0))),
                                data = test_loop, control = nls.control(warnOnly = TRUE), start = list(R0 = 1, E0 = 100))
          if(is.numeric(summary(regression_nls)$parameters["E0",2])) {
            E0_RSE <- append(E0_RSE, summary(regression_nls)$parameters["E0",2])
            E0_date <- append(E0_date, i + days(win_len - 1))
            E0_collection <- append(E0_collection, coef(regression_nls)[2])
          } else {print("produce NA of E0")}
        })
      } else {print("temp range < 5")} 
    } else {print("window < 6 points")}
  }
  E0_collection <- as.data.frame(E0_collection)
  E0_RSE <- as.data.frame(E0_RSE)
  E0_date <- as.data.frame(E0_date)
  E0_data_notFiltered <- data.frame(E0_collection, E0_RSE, E0_date)
  return(E0_data_notFiltered)
}

R0_regression <- function(df, E0, win_len, overlap_len) {
  R0_collection <- c()
  R0_RSE <- c()
  R0_date <- c()
  date_list <- seq(as.Date(unique(df$Time)[1]),
                   as.Date(unique(df$Time)[length(unique(df$Time))]), 
                   by = win_len - overlap_len)
  for( i in as.list(date_list)) {
    test_loop = subset(df, as.Date(Time) >= i & as.Date(Time) <= i + days(win_len - 1))
    if(nrow(test_loop) >= 6) {
      time <- centroid_day(test_loop, i)
      #time <- test_loop[nrow(test_loop), "Time_full"]
      try({
        regression_nls <- nls(NEE_VUT_REF ~ exp(E0 * (1/(Tref - T0) - 1/(TA_1_1_1 + CtoK - T0)))*R0,
                              data = test_loop, control = nls.control(warnOnly = TRUE), start = list(R0 = 1))
        if(is.numeric(summary(regression_nls)$parameters["R0",2])) {
          R0_RSE <- append(R0_RSE, summary(regression_nls)$parameters["R0",2])
          R0_collection <- append(R0_collection, coef(regression_nls)[1])
          R0_date <- append(R0_date, time)
        }
      })
    } else {print("less than 6 points available")}
  }
  R0_collection <- as.data.frame(R0_collection)
  R0_RSE <- as.data.frame(R0_RSE)
  R0_date <- as.data.frame(R0_date)
  R0_df <- data.frame(R0_collection, R0_RSE, R0_date)
  return(R0_df)
}

RECO_estimation <- function(df, E0) {
  df$RECO_NT_ori <- exp(E0 * (1/(Tref - T0) - 1/(df$TA_1_1_1  + CtoK - T0)))*(df$R0_interpolated)
  return(df)
}

```

#testing Russ Scott's site-cont
```{r}
df <- read.csv("/Users/ngocnguyen/Downloads/AMF_US-CMW_BASE-BADM_2-5/AMF_US-CMW_BASE_HH_2-5.csv", skip = 2)
df_labelled <- read.csv("/Users/ngocnguyen/Downloads/AMF_US-CMW_BASE-BADM_2-5/US-CMW-pulse.csv")

df_ref <- (time_convert(df))
df_ref$Time <- as.Date(df_ref$Time)
df_EF <- daytimeEF_df_BASE(df_ref)
df_full <- full_join(df_ref, df_EF, by= "Time")

# df_labelled <- time_adjust(df_labelled)
# 
# #mark df_labelled Day and Night
# df_labelled$NIGHT  <- 0
# df_labelled$NIGHT[which(df_labelled$SW_IN < 20)] <- 1
# 
# df_night <- subset(df_labelled, NIGHT == 1 & TA_1_1_1 > -9999)
# df_regression <- temp_regression(df_night, 15, 5)
# E0_long <- E0_calculation(df_regression)
# R0_df <- R0_regression(df_night, E0_long, 4, 1)
# R0_interpolate_df <- R0_interpolate(df_labelled, R0_df)
# RECO_estimation_df <- RECO_estimation(R0_interpolate_df, E0_long)

png("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Figures/19_sites_manual/US-CMW-2015.png",  width     = 10,
    height    = 2,
    units     = "in",
    res       = 1200,
    pointsize = 4)

for(i in unique(df_labelled$Year)) {
  df_sub <- subset(df_labelled, Year == i & NEE_PI > -9999 & RECO_PI > -9999 & Daily_EF > 0 & Daily_EF < 1 & P >= 0)
  
  print(ggplot(df_sub, aes(x = as.POSIXct(Time_full))) +
      geom_point(aes(y = NEE_PI, color = as.factor(pulse_cluster_y)), size = 0.15) +
      #geom_bar(aes(y = P/5), stat = "identity", color = "red", size = 0.3) +
      #geom_line(aes(y = Daily_EF*10), color = "red", size = 0.3) +
      geom_line(aes(y = RECO_NT_ori), color = "black", size = 0.1) +
      theme_bw() +
        theme(
            axis.text.y = element_text(size = 15),
            axis.text.x = element_text(size = 15),
            axis.title.y = element_text(size = 15),
            axis.title.y.right = element_text(size = 15, color = "white"),
            legend.key.size = unit(1, 'cm'),
            legend.text = element_text(size=13),
            panel.grid.major = element_blank() , 
            panel.grid.minor = element_blank()) +
      scale_fill_manual(values=c("#3c5488", "#00a087", "#f39b7f", "#f45b7f", "#f50b7f")) +
    scale_color_manual(values=c("#3c5488", "#00a087", "#f39b7f", "#f45b7f", "#f50b7f")) +
    guides(fill=guide_legend(title="New Legend Title")) +
    ylab(expression("Half-Hour NEE (umol "*CO[2]*" "*m^-2*s^-1*")")) + xlab("Time"))
}

```


```{r}
png("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Figures/19_sites_manual/Precipitation_underestimation_ES-Abr.png",  width     = 10,
    height    = 2,
    units     = "in",
    res       = 1200,
    pointsize = 4)
print(ggplot(subset(update, Year == 2017 & NEE_VUT_REF_QC == 0 & NIGHT == 0 & P_F > -9999), aes(x = as.POSIXct(Time_full))) +
        geom_bar(stat = "identity", aes(y = P_F), color = "red") +
    #geom_point(aes(y = NEE_VUT_REF, color = as.factor(pulse_cluster)), size = 0.25) +
          #geom_line(aes(y = RECO_pulse), color = "red", size = 0.3) +
    #geom_line(aes(y = NEE_VUT_REF), color = "black", size = 0.3) +
    theme_bw() +
      theme(
          axis.text.y = element_text(size = 15),
          axis.text.x = element_text(size = 15),
          axis.title.y = element_text(size = 15),
          axis.title.y.right = element_text(size = 15, color = "white"),
          legend.key.size = unit(1, 'cm'),
          legend.text = element_text(size=13),
          panel.grid.major = element_blank() , 
          panel.grid.minor = element_blank()) +
    scale_fill_manual(values=c("#3c5488", "#00a087", "#f39b7f", "#f45b7f", "#f50b7f")) +
  scale_color_manual(values=c("#3c5488", "#00a087", "#f39b7f", "#f45b7f", "#f50b7f")) +
  guides(fill=guide_legend(title="New Legend Title")) +
  ylab(expression("Half-Hour NEE (umol "*CO[2]*" "*m^-2*s^-1*")")) + xlab("Time"))
dev.off()
```

```{r}
# Load necessary libraries
library(ggplot2)
#install.packages("gganimate")
library(gganimate)
library(dplyr)

# Load your data
data <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/bias_correct_df_Aug18/ES-LM1.csv")

# Filter the data
filtered_data <- subset(data, Year == 2017 & NEE_VUT_REF_QC == 0)
# Create the base plot
p <- ggplot(filtered_data, aes(x = Time)) +
    geom_point(aes(y = NEE_VUT_REF, color = as.factor(pulse_cluster)), size = 0.25) +
    geom_line(aes(y = RECO_pulse), color = "red", size = 0.3) +
    geom_line(aes(y = RECO_NT_VUT_REF), color = "black", size = 0.3) +
    theme_bw() +
    theme(
        axis.text.y = element_text(size = 15),
        axis.text.x = element_text(size = 15),
        axis.title.y = element_text(size = 15),
        axis.title.y.right = element_text(size = 15, color = "white"),
        legend.key.size = unit(1, 'cm'),
        legend.text = element_text(size=13),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) +
    scale_fill_manual(values=c("#3c5488", "#00a087", "#f39b7f", "#f45b7f", "#f50b7f")) +
    scale_color_manual(values=c("#3c5488", "#00a087", "#f39b7f", "#f45b7f", "#f50b7f")) +
    guides(fill=guide_legend(title="New Legend Title")) +
    ylab(expression("Half-Hour NEE (umol "*CO[2]*" "*m^-2*s^-1*")")) + 
    xlab("Time")


```




#Calculating correction factor by fitting the curve to all points
```{r}
##df_manual is the data file that have 3 columns: SOP (Start of Pulse), EOP (End of Pulse), and SITE_ID. df_manual is manually labelled pulses. You can replace it with your ML labelled pulses
df_manual <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Data/Birch effect Manual Label - shortcut.csv")
df_manual$SOP <- as.Date(df_manual$SOP)
df_manual$EOP <- as.Date(df_manual$EOP)

##For pulse that does not pass the significant test, applying the mean decaying rate of each site (Column 'decay_factor' and 'decay_factor_p' in the metadata file)
#open df_stat to import decay_factor
df_stat <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/decay_stat_all_site.csv")

#original data of each site (uncorrected)
dir <- "/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Data/HH_data_time_converted/"

file_list <- list.files(dir)

for(j in 1:length(file_list)) {
  ##Reading each file
  file_path <- paste(dir, file_list[j], sep = "")
  RECO_df <- read.csv(file_path, header = TRUE) 
  RECO_df$Time <- as.Date(RECO_df$Time)
  df_EF <- daytimeEF_df(RECO_df)
  RECO_df <- full_join(RECO_df, df_EF, by= "Time")
  RECO_df$Time <- as.Date(RECO_df$Time_full)
  #reading the pulse df for each site
  site_name <- substr(file_list[j], start = 1, stop = 6)
  df_manual_sub <- subset(df_manual, SITE_ID == site_name)
  
  decay_factor_composite <- subset(df_stat, SITE_ID == site_name)$decay_factor

  RECO_df_pulse_list <- list()
  fail_time <- c()
  for(i in 1:nrow(df_manual_sub)) {
    RECO_df_sub <- subset(RECO_df, as.Date(Time) %in% as.Date((df_manual_sub$SOP[i]: df_manual_sub$EOP[i])) & RECO_NT_VUT_REF > -9999 & NEE_VUT_REF_QC == 0)
    if(length(unique(RECO_df_sub$Time)) > 2) {
    #decay function for each pulse
    df_decay <- RECO_df_sub %>% group_by(Time) %>% summarise(across(c(NEE_VUT_REF, RECO_NT_VUT_REF), quantile_df))
    df_decay$Day_index <- 1:nrow(df_decay)
    df_decay$correction_factor <- 0
    
    tryCatch({
    fit <- nls(data = df_decay, NEE_VUT_REF ~ a * exp(-b * Day_index), 
               start = list(a = max(df_decay$NEE_VUT_REF), b = 0.1))
    ini_condition <- summary(fit)$coefficients[1, 1]
    decay_factor <- summary(fit)$coefficients[2,1]
    ini_condition_p <-  summary(fit)$coefficients[1, 4]
    decay_factor_p <- summary(fit)$coefficients[2,4]
    
    #if p value passes the signficant test --> use the individual pulse decay rate, not the site composite decay rate
    for(k in 1:(nrow(df_decay))) {
        if(ini_condition_p < 0.1 & decay_factor_p < 0.1) {
        correction_factor_0 <- max(c(df_decay$NEE_VUT_REF[1], df_decay$NEE_VUT_REF[2], na.rm = TRUE))/max(c(df_decay$RECO_NT_VUT_REF[1], df_decay$RECO_NT_VUT_REF[2], na.rm = TRUE))*exp(decay_factor)
        df_decay$correction_factor[k] <- correction_factor_0*exp(- decay_factor * k)
        } else {
        correction_factor_0 <- max(c(df_decay$NEE_VUT_REF[1], df_decay$NEE_VUT_REF[2], na.rm = TRUE))/max(c(df_decay$RECO_NT_VUT_REF[1], df_decay$RECO_NT_VUT_REF[2], na.rm = TRUE))*exp(decay_factor_composite)
        df_decay$correction_factor[k] <- correction_factor_0*exp(- decay_factor_composite * k)
        fail_time <- append(fail_time, df_decay$Time[1])
        }
      }
    }, error = function(err) { 
      
      for(k in 1:(nrow(df_decay))) {
        correction_factor_0 <- max(c(df_decay$NEE_VUT_REF[1], df_decay$NEE_VUT_REF[2], na.rm = TRUE))/max(c(df_decay$RECO_NT_VUT_REF[1], df_decay$RECO_NT_VUT_REF[2], na.rm = TRUE))*exp(decay_factor_composite)
        df_decay$correction_factor[k] <- correction_factor_0*exp(- decay_factor_composite * k)
      }
      })
      correction_factor_df <- df_decay[, c("Time", "correction_factor")]
      RECO_df_sub_new <- list(RECO_df_sub, correction_factor_df) %>% reduce(full_join, by = "Time")
      RECO_df_sub_new$RECO_pulse <- RECO_df_sub_new$RECO_NT_VUT_REF*RECO_df_sub_new$correction_factor
      RECO_df_pulse_list[[i]] <- RECO_df_sub_new
    } }

  all_pulse_df <- NULL
  for(i in 1:length(RECO_df_pulse_list)) {
    all_pulse_df <- rbind(all_pulse_df, RECO_df_pulse_list[[i]])
  }          

  all_pulse_df_sub <- subset(all_pulse_df[,c("Time_full", "correction_factor", "RECO_pulse")], correction_factor < 100)
           
  update_df <- list(all_pulse_df_sub, RECO_df) %>% reduce(full_join, by = "Time_full")   
  
  update_df$RECO_pulse[which(is.na(update_df$RECO_pulse))] <- update_df$RECO_NT_VUT_REF[which(is.na(update_df$RECO_pulse))]

  # #mark pulse date
pulse_date <- c()
  ##generate pulse dataset
for(i in 1:nrow(df_manual_sub)) {
  date <- as.Date((df_manual_sub$SOP[i]):df_manual_sub$EOP[i])
  pulse_date <- append(pulse_date, date)
}

update_df$pulse_cluster <- update_df$NEE_VUT_REF
update_df$pulse_cluster[which(update_df$Time %in% pulse_date)] <- 1
update_df$pulse_cluster[-which(update_df$Time %in% pulse_date)] <- 0

 write.csv(update_df, paste("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/DT_interpolated_datasets_all_pulse/", site_name, ".csv", sep = ""))
}
```

#mark small pulses and large pulses
```{r}
df_pulse_class <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/3vars_all_sites.csv")
df_pulse_class$SOP <- as.Date(df_pulse_class$SOP)

df_manual <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Data/Birch effect Manual Label - shortcut.csv")
df_manual$SOP <- as.Date(df_manual$SOP)
df_manual$EOP <- as.Date(df_manual$EOP)

#merge C_contribution with df_manual
df_new <- list(df_pulse_class, df_manual) %>% reduce(full_join, by = c("SOP", "SITE_ID"))
df_new_include <- df_new[, c("SITE_ID", "SOP", "EOP", "C_contribution")]

dir <- "/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/DT_interpolated_datasets_all_pulse/"
dir_2 <- "/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/pulse_cluster_ext/"

file_list <- list.files(dir)

for(j in 1:length(file_list)) {
  ##Reading each file
  file_path <- paste(dir, file_list[j], sep = "")
  RECO_df <- read.csv(file_path, header = TRUE) 
  RECO_df$Time <- as.Date(RECO_df$Time)
  #reading the pulse df for each site
  site_name <- substr(file_list[j], start = 1, stop = 6)
  df_manual_big <- subset(df_new_include, SITE_ID == site_name & C_contribution >= 0)
  df_manual_small <- subset(df_new_include, SITE_ID == site_name & C_contribution < 0)

  #mark pulse date
  pulse_date_big <- c()
    ##generate pulse dataset
  if(nrow(df_manual_big) != 0) {
  for(i in 1:nrow(df_manual_big)) {
    date <- as.Date((df_manual_big$SOP[i]):df_manual_big$EOP[i])
    pulse_date_big <- append(pulse_date_big, date)
  }
  } else {
    pulse_date_big <- c()
  }
  
  #mark pulse date
  pulse_date_small <- c()
    ##generate pulse dataset
  if(nrow(df_manual_small) != 0) {
  for(i in 1:nrow(df_manual_small)) {
    date <- as.Date((df_manual_small$SOP[i]):df_manual_small$EOP[i])
    pulse_date_small <- append(pulse_date_small, date)
  } 
  } else {
    pulse_date_small <- c()
  }
  
  RECO_df$pulse_cluster_ext <- 0
  RECO_df$pulse_cluster_ext[which(RECO_df$Time %in% pulse_date_big)] <- 1
  RECO_df$pulse_cluster_ext[which(RECO_df$Time %in% pulse_date_small)] <- -1

  write.csv(RECO_df, paste(dir_2, site_name, ".csv", sep = ""))
}
```


#fraction of small/big pulses for each site
```{r}


fraction_big <- function(x) {
  big_num <- nrow(subset(x, C_contribution >= 0))
  fraction <- big_num/nrow(x)
  return(fraction)
}

fraction_list <- c()
SITE_list <- c()

for(i in unique(df_pulse_class$SITE_ID)) {
  sub_df <- subset(df_pulse_class, SITE_ID == i)
  fraction_list <- append(fraction_list, fraction_big(sub_df))
  SITE_list <- append(SITE_list, i)
}

df_final <- data.frame(SITE_list, fraction_list)
subset(df_final, fraction_list < 0.5)
```




#UPdate fail pulses 
```{r}
# index_remove <- c(which(df_manual$SITE_ID == "US-Jo2" & as.character(df_manual$SOP) %in% c("2018-06-15", "2018-02-15", "2018-09-05")),
# which(df_manual$SITE_ID == "US-Rws" & as.character(df_manual$SOP) %in% c("2017-01-04")),
# which(df_manual$SITE_ID == "US-SRG" & as.character(df_manual$SOP) %in% c("2021-07-01")),
# which(df_manual$SITE_ID == "US-Ton" & as.character(df_manual$SOP) %in% c("2003-10-30")),
# which(df_manual$SITE_ID == "US-Var" & as.character(df_manual$SOP) %in% c("2014-02-07", "2016-10-15", "2006-11-02", "2007-10-10")),
# which(df_manual$SITE_ID == "US-Whs" & as.character(df_manual$SOP) %in% c("2019-01-01", "2018-06-14")),
# which(df_manual$SITE_ID == "US-Wjs" & as.character(df_manual$SOP) %in% c("2011-02-01")))

# site_name = "US-Wjs"
# df <- read.csv(paste("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/DT_interpolated_datasets_all_pulse/", site_name, ".csv", sep = ""))
# 
# index_remove <- which(as.Date(df$Time) %in% as.Date(c(as.Date("2011-02-01"):as.Date("2011-02-09")
#                                                       
#                                      )))
#                       
# df$RECO_pulse[index_remove] <- df$RECO_NT_VUT_REF[index_remove]
# 
# write.csv(df, paste("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/DT_interpolated_datasets_all_pulse/", site_name, ".csv", sep = ""))


```



#calculating the correction factor by forcing the decaying rate (to avoid underestimation of the last few days of pulses)

```{r}
df_manual <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Data/Birch effect Manual Label - shortcut.csv")
df_manual$SOP <- as.Date(df_manual$SOP)
df_manual$EOP <- as.Date(df_manual$EOP)

##for pulse that does not pass the significant test, applying the mean decaying function 
df_stat <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/decay_stat_all_site.csv")

dir <- "/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Data/HH_data_time_converted/"
file_list <- list.files(dir)
no_pulse <- c()

for(j in 1:length(file_list)) {
  ##Reading each file
  file_path <- paste(dir, file_list[j], sep = "")
  RECO_df <- read.csv(file_path, header = TRUE)
  RECO_df$Time <- as.Date(RECO_df$Time)
  RECO_df$Time <- as.Date(RECO_df$Time)
  df_EF <- daytimeEF_df(RECO_df)
  RECO_df <- full_join(RECO_df, df_EF, by= "Time")
  #reading the pulse df for each site
  site_name <- substr(file_list[j], start = 1, stop = 6)
  df_manual_sub <- subset(df_manual, SITE_ID == site_name)

  RECO_df_pulse_list <- list()
  for(i in 1:nrow(df_manual_sub)) {
    RECO_df_sub <- subset(RECO_df, as.Date(Time) %in% as.Date((df_manual_sub$SOP[i] : df_manual_sub$EOP[i])) & RECO_NT_VUT_REF > -9999 & NEE_VUT_REF_QC == 0)
    if(length(unique(RECO_df_sub$Time)) > 2) {
    #decay function for each pulse
    df_decay <- RECO_df_sub %>% group_by(Time) %>% summarise(across(c(NEE_VUT_REF, RECO_NT_VUT_REF, GPP_NT_VUT_REF), max))
    df_decay$Day_index <- 0:(nrow(df_decay) - 1)
    df_decay$correction_factor <- 0
    

   decay_factor <- -log(df_decay$NEE_VUT_REF[nrow(df_decay)]/df_decay$NEE_VUT_REF[1])/nrow(df_decay)
   
   df_decay$correction_factor <- (df_decay$NEE_VUT_REF[1] + df_decay$GPP_NT_VUT_REF[1])/df_decay$RECO_NT_VUT_REF[1]*exp(- decay_factor*df_decay$Day_index)
  
   correction_factor_df <- df_decay[, c("Time", "correction_factor")]
     
  RECO_df_sub_new <- list(RECO_df_sub, correction_factor_df) %>% reduce(full_join, by = "Time")
  RECO_df_sub_new$RECO_pulse <- RECO_df_sub_new$RECO_NT_VUT_REF*(1 + RECO_df_sub_new$correction_factor)/2
  RECO_df_pulse_list[[i]] <- RECO_df_sub_new
    } }

  all_pulse_df <- NULL
  for(i in 1:length(RECO_df_pulse_list)) {
    all_pulse_df <- rbind(all_pulse_df, RECO_df_pulse_list[[i]])
  }          

  all_pulse_df_sub <- subset(all_pulse_df[,c("Time_full", "correction_factor", "RECO_pulse")], correction_factor < 1000)
           
  update_df <- list(all_pulse_df_sub, RECO_df) %>% reduce(full_join, by = "Time_full")   
  
  update_df$RECO_pulse[which(is.na(update_df$RECO_pulse))] <- update_df$RECO_NT_VUT_REF[which(is.na(update_df$RECO_pulse))]

  # #mark pulse date
pulse_date <- c()
  ##generate pulse dataset
for(i in 1:nrow(df_manual_sub)) {
  date <- as.Date((df_manual_sub$SOP[i]):df_manual_sub$EOP[i])
  pulse_date <- append(pulse_date, date)
}

update_df$pulse_cluster <- update_df$NEE_VUT_REF
update_df$pulse_cluster[which(update_df$Time %in% pulse_date)] <- 1
update_df$pulse_cluster[-which(update_df$Time %in% pulse_date)] <- 0

  write.csv(update_df, paste("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/DT_interpolate_k_forcing/", site_name, ".csv", sep = ""))
}

```

#Visualization
```{r}
site_name = "ES-Abr"
update_df <- read.csv(paste("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/pulse_intensity_filter_binary/", site_name, ".csv", sep = ""))

Year <- unique(update_df$Year)

daily_df <- function(df) {
  df <- df %>% group_by(Time) %>%  filter(n() >= 10) %>%
    summarise(across(c(Year, NEE_VUT_REF, RECO_pulse, RECO_NT_VUT_REF, pulse_cluster), mean))
  return(df)
}
update_df <- subset(update_df, NEE_VUT_REF_QC == 0 & NIGHT == 1)
update_df_daily <- daily_df(update_df)
update_df_daily$pulse_cluster <- as.factor(update_df_daily$pulse_cluster)

#pdf(paste("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Figures/19_sites_manual/", site_name, "GPP_NT_ts.pdf", sep = ""), width = 10, height = 5)
for(i in Year) {
print(ggplot(subset(update_df_daily, Year == i), aes(x = as.POSIXct(Time))) +
    geom_point(aes(y = NEE_VUT_REF, color = as.factor(pulse_cluster)), size = 0.4) +
      geom_hline(yintercept = 0, col = "red", linetype = "dashed") +
    #, color = 

    geom_point(aes(y = RECO_pulse), color = "red", size = 0.4) +
        geom_line(aes(y = RECO_NT_VUT_REF), color = "black") +
    # geom_line(aes(y = P_ERA*5 ), color = "black", size = 0.3) +
  #scale_y_continuous(sec.axis = sec_axis(~., name = "")) + 
    theme_bw() +
      theme(
        legend.position = "none",
          axis.text.y = element_text(size = 15),
          axis.text.x = element_text(size = 15),
          axis.title.y = element_text(size = 15),
          axis.title.y.right = element_text(size = 15, color = "white"),
          legend.key.size = unit(1, 'cm'),
          legend.text = element_text(size=13),
          panel.grid.major = element_blank() , 
          panel.grid.minor = element_blank()) +
    scale_fill_manual(values=c("#3c5488", "#00a087", "#f39b7f", "#f45b7f", "#f50b7f")) +
  scale_color_manual(values=c("#3c5488", "#00a087", "#f39b7f", "#f45b7f", "#f50b7f")) +
  guides(fill=guide_legend(title="New Legend Title")) +
  ylab(expression("Half-hourly GPP_FluxPulse (umol "*CO[2]*" "*m^-2*s^-1*")")) + xlab("Time"))
}
#dev.off()


ggplot(update_df, aes(x = RECO_NT_VUT_REF, y = NEE_VUT_REF)) + geom_point(size = 0.5) + 
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "blue") + 
facet_wrap(~ pulse_cluster) + 
theme_bw()

ggplot(update_df, aes(x = RECO_pulse, y = NEE_VUT_REF)) + geom_point(size = 0.5) + 
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "blue") + 
facet_wrap(~ pulse_cluster) + 
theme_bw()

summary(lm(RECO_NT_VUT_REF ~ NEE_VUT_REF, subset(update_df, pulse_cluster == 0)))$r.squared
summary(lm(RECO_NT_VUT_REF ~ NEE_VUT_REF, subset(update_df, pulse_cluster == 1)))$r.squared
summary(lm(RECO_pulse ~ NEE_VUT_REF, subset(update_df, pulse_cluster == 1)))$r.squared

mrse <- function(x, y) {
  mrse <- sqrt(mean((x - y)^2))
return(mrse)
}

mrse(update_df$NEE_VUT_REF, update_df$RECO_pulse)
mrse(update_df$NEE_VUT_REF, update_df$RECO_NT_VUT_REF)
```
#compare RECO_Pulse/RECO_NT_VUT_REF with NEE_VUT_REF during pulses    
#Rule: Reco >= NEE
```{r}
update_df <- read.csv("/Users/ngocnguyen/Downloads/US-Ton.csv")
Year <- unique(update_df$Year)
update_df <- update_df[order(update_df$Time_full), ]
print(ggplot(subset(update_df, Year == 2017), aes(x = as.POSIXct(Time_full))) +
    geom_point(aes(y = NEE_VUT_REF, color = as.factor(NIGHT)), size = 0.25) +
    geom_line(aes(y = RECO_pulse), color = "red", size = 0.5) +
    geom_line(aes(y = RECO_NT_VUT_REF), color = "black", size = 0.3) +
  #scale_y_continuous(sec.axis = sec_axis(~., name = "")) + 
    theme_bw() +
      theme(
          axis.text.y = element_text(size = 15),
          axis.text.x = element_text(size = 15),
          axis.title.y = element_text(size = 15),
          axis.title.y.right = element_text(size = 15, color = "white"),
          legend.key.size = unit(1, 'cm'),
          legend.text = element_text(size=13),
          panel.grid.major = element_blank() , 
          panel.grid.minor = element_blank()) +
    scale_fill_manual(values=c("#3c5488", "#00a087", "#f39b7f", "#f45b7f", "#f50b7f")) +
  scale_color_manual(values=c("#3c5488", "#00a087", "#f39b7f", "#f45b7f", "#f50b7f")) +
  guides(fill=guide_legend(title="New Legend Title")) +
  ylab(expression("Half-Hour NEE (umol "*CO[2]*" "*m^-2*s^-1*")")) + xlab("Time"))
```



```{r}
png("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Figures/19_sites_manual/EF_time_series_ES-Abr-2017-sub.png",  width     = 6,
    height    = 1.75,
    units     = "in",
    res       = 1200,
    pointsize = 4)

ggplot(subset(update_df, Year == 2017 & Month > 5), aes(x = as.POSIXct(Time_full))) +
    geom_line(aes(y = Daily_EF), color = "black") +
  geom_line(aes(y = RECO_pulse/10), color = "black") +
    geom_bar(data = subset(update_df, Year == 2017 & Month > 5), stat = "identity", aes(y = P_ERA/3), color = "red", position = "dodge") +
    scale_y_continuous(sec.axis = sec_axis(~.*3, name = "Precipitation (mm/half-hour)")) + 
    theme_bw() +
    theme(axis.text.y.right = element_text(color = "red"),
          axis.title.y.right = element_text(color = "red"),
          axis.text.y = element_text(size = 15),
          axis.text.x = element_text(size = 15),
          axis.title.y = element_text(size = 15),
          axis.title.x = element_text(size = 15),
          panel.grid.major = element_blank() , 
          panel.grid.minor = element_blank()) + 
    ylab("Evaporative Fraction (EF)") + xlab("Time")
dev.off()
```

```{r}
png("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Figures/19_sites_manual/GPP_underestimation_ex_S11.png",  width     = 10,
    height    = 4,
    units     = "in",
    res       = 1200,
    pointsize = 4)

print(ggplot(subset(update_df, Year == 2017), aes(x = as.POSIXct(Time_full))) +
    geom_point(aes(y = NEE_VUT_REF, color = as.factor(pulse_cluster)), size = 0.25) +
    geom_line(aes(y = GPP_DT_VUT_REF), color = "red", size = 0.3) +
  #scale_y_continuous(sec.axis = sec_axis(~., name = "")) + 
    theme_bw() +
      theme(
          axis.text.y = element_text(size = 15),
          axis.text.x = element_text(size = 15),
          axis.title.y = element_text(size = 15),
          axis.title.y.right = element_text(size = 15, color = "white"),
          legend.key.size = unit(1, 'cm'),
          legend.text = element_text(size=13),
          panel.grid.major = element_blank() , 
          panel.grid.minor = element_blank()) +
    scale_fill_manual(values=c("#3c5488", "#00a087", "#f39b7f", "#f45b7f", "#f50b7f")) +
  scale_color_manual(values=c("#3c5488", "#00a087", "#f39b7f", "#f45b7f", "#f50b7f")) +
  guides(fill=guide_legend(title="New Legend Title")) +
  ylab(expression("Half-Hour NEE (umol "*CO[2]*" "*m^-2*s^-1*")")) + xlab("Time"))
dev.off()
```



##DT interpolated: additive function instead
```{r}
#Calculate enhancement for the first day
site_name = "US-Var"
update_df <- read.csv(paste("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Results/Datasets/DT_interpolated_datasets_all_pulse/", site_name, ".csv", sep = ""))
#do partition again 

  site_train <- subset(train_df(update_df), TS_F_MDS_1_QC == 0)
  ##Run temp-regression
  E0_df <- temp_regression(site_train, 14, 5)
  E0 <- E0_calculation(E0_df)
  R0_df <- R0_regression(site_train, E0, 1, 0)
  R0_interpolated <- R0_interpolate(update_df, R0_df)
  RECO_df <- RECO_estimation(R0_interpolated, E0)
update_df = RECO_df

df_manual <- read.csv("/Users/ngocnguyen/Library/CloudStorage/Box-Box/Projects/Birch_effect_paper/Data/Birch effect Manual Label - shortcut.csv")
df_manual$SOP <- as.Date(df_manual$SOP)
df_manual$EOP <- as.Date(df_manual$EOP)
df_manual_sub <- subset(df_manual, SITE_ID == site_name)

df_list <- list()
for(i in 1:nrow(df_manual_sub)) {
  tryCatch({
    RECO_df_sub <- subset(update_df, as.Date(Time) %in% as.Date((as.Date(df_manual_sub$SOP[i] - 2) : df_manual_sub$EOP[i])) & RECO_NT_VUT_REF > -9999 & NEE_VUT_REF_QC == 0)

    #decay function for each pulse
    df_decay <- RECO_df_sub %>% group_by(Time) %>% summarise(across(c(NEE_VUT_REF, RECO_NT_VUT_REF, RECO_NT_ori, GPP_NT_VUT_REF), mean))
    df_decay$Day_index <- -1:(nrow(df_decay)-2)    
    df_decay$day_sqrt <- df_decay$Day_index**0.5
    #how much are underestimated in the first day

      max_first <- max(subset(df_decay, Day_index == 0)$GPP_NT_VUT_REF - subset(df_decay, Day_index == 1)$GPP_NT_VUT_REF,
                       subset(df_decay, Day_index == 0)$GPP_NT_VUT_REF - subset(df_decay, Day_index == 2)$GPP_NT_VUT_REF,
                       subset(df_decay, Day_index == -1)$GPP_NT_VUT_REF - subset(df_decay, Day_index == 1)$GPP_NT_VUT_REF,
                       subset(df_decay, Day_index == -1)$GPP_NT_VUT_REF - subset(df_decay, Day_index == 2)$GPP_NT_VUT_REF
                       )
        enhancement_1 <- max_first/subset(df_decay, Day_index == 1)$RECO_NT_VUT_REF
    #decay curve
    
    fit <- nls(data = df_decay[-c(1, 2), ], NEE_VUT_REF ~ a * exp(-b * (day_sqrt)), 
               start = list(a = max(df_decay[-c(1, 2), ]$NEE_VUT_REF), b = 0.1))
    ini_condition <- summary(fit)$coefficients[1, 1]
    decay_factor <- summary(fit)$coefficients[2,1]
    ini_condition_p <-  summary(fit)$coefficients[1, 4]
    decay_factor_p <- summary(fit)$coefficients[2,4]
    
   
    if(ini_condition_p < 1 & decay_factor_p < 1) {
          decay_factor_final <- decay_factor
          enhancement_follow <- 1 + enhancement_1*exp(-decay_factor_final*(df_decay$day_sqrt - 1))
          df <- data.frame(df_decay$Time, enhancement_follow)[-c(1, 2),]
          df_list[[i]] <- df 
    }
    
    }, error = function(err) { 
      df_list[[i]] <- NA
      })
}




  all_enhancement_df <- NULL
  for(i in 1:length(df_list)) {
    all_enhancement_df <- rbind(all_enhancement_df, df_list[[i]])
  }          

  colnames(all_enhancement_df) <- c("Time", "Enhancement")

  
df_new <- list(update_df,all_enhancement_df) %>% reduce(full_join, by = "Time")
df_new$RECO_additive <- df_new$RECO_NT_ori*df_new$Enhancement
#df_new <- subset(df_new, Enhancement < 100000)





ggplot(subset(update_df, NEE_VUT_REF_QC == 0  & NIGHT ==1 & pulse_cluster == 1), aes(RECO_pulse, NEE_VUT_REF, color = as.factor(pulse_cluster))) + geom_point(size = 0.2) + geom_abline() + theme_bw()

ggplot(subset(update_df, NEE_VUT_REF_QC == 0  & NIGHT == 1 & pulse_cluster == 1), aes(RECO_NT_VUT_REF, NEE_VUT_REF, color = as.factor(pulse_cluster))) + geom_point(size = 0.2) + geom_abline() + theme_bw()


for(i in 1:nrow(df_manual_sub)) {
    RECO_df_sub <- subset(update_df, as.Date(Time) %in% as.Date((as.Date(df_manual_sub$SOP[i]) : df_manual_sub$EOP[i])) & RECO_NT_VUT_REF > -9999 & NEE_VUT_REF_QC == 0)
    df_decay <- RECO_df_sub %>% group_by(Time) %>% summarise(across(c(NEE_VUT_REF, RECO_NT_VUT_REF, GPP_NT_VUT_REF), mean))
    df_decay$Day_index <- 1:(nrow(df_decay))    
    df_decay$day_sqrt <- df_decay$Day_index**0.5
    
    print(ggplot(df_decay, aes(x = day_sqrt, y = NEE_VUT_REF)) +
    geom_point() + geom_smooth(method = "lm", se = FALSE) +
    theme_bw() +
      theme(
          axis.text.y = element_text(size = 15),
          axis.text.x = element_text(size = 15),
          axis.title.y = element_text(size = 15),
          axis.title.y.right = element_text(size = 15, color = "white"),
          legend.key.size = unit(1, 'cm'),
          legend.text = element_text(size=13),
          panel.grid.major = element_blank() , 
          panel.grid.minor = element_blank()) +
    scale_fill_manual(values=c("#3c5488", "#00a087", "#f39b7f", "#f45b7f", "#f50b7f")) +
  scale_color_manual(values=c("#3c5488", "#00a087", "#f39b7f", "#f45b7f", "#f50b7f")) +
  guides(fill=guide_legend(title="New Legend Title")) +
  ylab(expression("Half-Hour NEE (umol "*CO[2]*" "*m^-2*s^-1*")")) + xlab("Time"))
    
}
```


```{r}
print(ggplot(subset(update_df, Year == 2017 & NEE_VUT_REF_QC == 0), aes(x = as.POSIXct(Time_full))) +
    geom_point(aes(y = NEE_VUT_REF, color = as.factor(NIGHT)), size = 0.25) +
          geom_line(aes(y = RECO_pulse), color = "red", size = 0.3) +
    geom_line(aes(y = RECO_NT_VUT_REF), color = "black", size = 0.3) +
    theme_bw() +
      theme(
          axis.text.y = element_text(size = 15),
          axis.text.x = element_text(size = 15),
          axis.title.y = element_text(size = 15),
          axis.title.y.right = element_text(size = 15, color = "white"),
          legend.key.size = unit(1, 'cm'),
          legend.text = element_text(size=13),
          panel.grid.major = element_blank() , 
          panel.grid.minor = element_blank()) +
    scale_fill_manual(values=c("#3c5488", "#00a087", "#f39b7f", "#f45b7f", "#f50b7f")) +
  scale_color_manual(values=c("#3c5488", "#00a087", "#f39b7f", "#f45b7f", "#f50b7f")) +
  guides(fill=guide_legend(title="New Legend Title")) +
  ylab(expression("Half-Hour NEE (umol "*CO[2]*" "*m^-2*s^-1*")")) + xlab("Time"))

print(ggplot(subset(update_df_2, Year == 2021 & NEE_VUT_REF_QC == 0), aes(x = as.POSIXct(Time_full))) +
    geom_point(aes(y = NEE_VUT_REF, color = as.factor(NIGHT)), size = 0.25) +
          geom_line(aes(y = RECO_pulse), color = "red", size = 0.3) +
    geom_line(aes(y = RECO_NT_VUT_REF), color = "black", size = 0.3) +
    theme_bw() +
      theme(
          axis.text.y = element_text(size = 15),
          axis.text.x = element_text(size = 15),
          axis.title.y = element_text(size = 15),
          axis.title.y.right = element_text(size = 15, color = "white"),
          legend.key.size = unit(1, 'cm'),
          legend.text = element_text(size=13),
          panel.grid.major = element_blank() , 
          panel.grid.minor = element_blank()) +
    scale_fill_manual(values=c("#3c5488", "#00a087", "#f39b7f", "#f45b7f", "#f50b7f")) +
  scale_color_manual(values=c("#3c5488", "#00a087", "#f39b7f", "#f45b7f", "#f50b7f")) +
  guides(fill=guide_legend(title="New Legend Title")) +
  ylab(expression("Half-Hour NEE (umol "*CO[2]*" "*m^-2*s^-1*")")) + xlab("Time"))

```











